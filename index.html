<html>
<body>
<pre>
ILSVRC history:
Classification: Alexnet, Zeiler & Fergus, GoogLeNet (VGG), ResNet 
Localization: Alexnet, OverFeat, VGG, ResNet + RPN
Detection: ?, OverFeat, UvA-Euvision, GoogLeNet, ResNet + Faster-R-CNN

Datasets:
NORB
Caltech 101 / 256
CIFAR 10 / 100
CIFAR 10 - 50K training images, 10K testing images over 10 classes

MNIST
Imagenet - 15 million labeled images / 22000 categories
ILSVRC - 1.2 million labeled images, 50000 validation, 150000 testing. ~1000 images in each of 1000 categories
ILSVRC 2010 - test set labels are available

ILSVRC 2013 - localization considered correct only if > 50% overlap (intersection-over-union). For localization, a single box is
predicted for the predicted class.
detection challenge may have zero objects (i.e. background class). background class makes detection harder.
localization and classification share data. detection has few more data points: images where the objects are smaller, images where
certain objects are absent (can be used for background class)
classification winner was clarifai ~11% (results in overfeat paper)
localization winner was overfeat 29.9%

ILSVRC 2012 and 2013 have the same train and test data? (mentioned in overfeat)

PASCAL VOC 2007 - ~10K images for classification task; 20 object categories; mAP metric is used
PASCAL VOC 2012 - ~22K images for classification task; 20 object categories; mAP metric is used
Caltech 101 - 9K images in 102 categories (101 + a background class)
Caltech 256 - 31K images in 257 categories
PASCAL VOC 2012 Action classification - predict action given a bounding box around a person performing an action. 4.6K images and 11 classes. mAP metric is used.

The way caltech dataset is typically evaluated: Generate random train/test splits and report average performance across them. Mean class recall is used since it compensates for different number of test images per class.


Preprocessing:


Regularization:
Data augmentation - adding noise, cropping, flipping, altering intensity of RGB images
Alexnet style RGB intensity alteration - collect all RGB values (3D vector) from all training images. PCA to compute eigenvectors 
and eigenvalues. Alter each training image by adding all three PCA directions scaled by their eigenvalue * N(0,0.1). Note: all 
pixels in image use the same gaussian sample but different images use different ones.
Dropout - reduces co-adaptation of features. increases the number of iterations needed to converge. e.g. w/ 0.5, alexnet needs
roughly 2X iterations (note: alexnet runs dropout only on two FC layers but these layers have most of the parameters)

Misc:
LRN - local response normalization - post Relu; normalize the activation by L2 norm of activations from 
neighboring kernels (n/2 on either side) raised to beta (hyperparameter). 
This ties the activations together, e.g. two activations can't both be high
Image similarity can be determined by L2 norm on the last hidden layer's representation of image. E.g. when used on ILSVRC, this 
retrieves similar images which in pixel-level-L2 would be far off.
Convolutions over entire image is more efficient than computing entire pipeline for each sliding window due to convolutions 
reusing overlapping computations (see fig 5 of Overfeat)

Optimization:
GD:
SGD:
Momentum:
Nesterov Momentum:
AdaGrad:
RMSProp:
Adam:


conv layer: for 1 based indexing: op dimension = floor( 1 + (ip dimension - kernel dimension + padding) / stride )

Title: ImageNet Classification with Deep Convolutional Neural Networks

Notes: 
60 million params, 650K neurons, dropout regularization, Relu
ILSVRC 2010 & 2012
1.2 million labeled images
5-6 days on 2 GTX580
rescaled img such that shorter side of image is 256. central 256x256 cropped out
subtracted the mean value, over training set, per pixel
LRN - local response normalization - reduces error rates by ~1.2%
Overlapping Pooling - makes it slightly harder to overfit

Architecture:
Relu activations throughout
ip layer: 224 X 224 X 3
kernels:  11 X 11 ; stride 4
layer 2:  55 X 55 X 96
LRN: neighborhood 5, beta 0.75, alpha, k
max pool: 3 x 3; stride 2
kernels:  5 X 5 ; stride 
layer 3:  27 X 27 X 256
LRN: same as above
max pool: 3 X 3; stride 2
kernels: 3 X 3 ; stride
layer 4: 13 X 13 X 384
kernels: 3 X 3 ; stride 
layer 5: 13 X 13 X 384
kernels: 3 X 3 ; stride
layer 6: 13 X 13 X 256
max pool: 3 X 3; stride 2
dropout: 0.5
layer 7: FC 4096
dropout: 0.5
layer 8: FC 4096
op layer: softmax 1000
Data augmentation - 2048 X increase in training set by cropping random 224x224 regions + their horizontal flips.
10 X during testing by considering the four corners, center + their horizontal flips
Learning: SGD, 0.9 momentum, mini-batch 128, 0.0005 weight decay. learning rate set to 0.01 and reduced 10X manually based on 
validation error. during training it went through 3 reductions. 
weight decay is not just regularizing. it helps reduce training error as well.
Initialization: weights are N(0,0.01). biases in layers 3, 5, 6 are set to 1; remaining are set to 0.
Training consumed 90 cycles of the entire training set (1.2 million images)

Results:
ILSVRC 2010: top 1: 37.5%; top 5: 17.0%; top 5 w/o avg-ing over 10 patches: 18.3%
ILSVRC 2012: top 5: 18.2%; 5 CNN avg top 5: 16.4%
Removing any of the conv layers results in 2% loss on top 1 performance.

OverFeat:
single convnet for classification, localization and detection.
avoid training on background class.
ILSVRC 2012 and 2013. Winner of 2013 localization.

Training:
Trained on ILSVRC 2012 (1.2 million images)
Image is rescaled so the smaller dimension is of 256 pixels. 
Extract 5 random crops (221 X 221) and feed them + their horizontal flips in mini-batches of 128.
weights initialized to N(0,0.01)
SGD with momentum 0.6, weight decay 0.00001
Learning rate: 0.05 and reduced 2X at (30, 50, 60, 70, 80) epochs

Architecture:
Training treats architecture as non-spatial (i.e. 1x1 output) but inference treats it as spatial.
Relu activations throughout
(typo in their paper on the first two layers of the fast model?)
Fast model:
ip layer: 221 X 221 X 3
kernels: 11 X 11; stride 4
max pool: 2 X 2; stride 2
layer 1: 28 X 28 X 96
kernels: 5 X 5; stride 1
max pool: 2 X 2; stride 2
layer 2: 12 X 12 X 256
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 12 X 12 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 4: 12 X 12 X 1024
padding: 1 all around
kernels: 3 X 3; stride 1
max pool: 2 X 2; stride 2
layer 5: 6 X 6 X 1024
FC:
layer 6: 1 X 1 X 3072
FC:
layer 7: 1 X 1 X 4096
softmax: 1000 classes

Accurate:
ip layer: 221 X 221 X 3
kernels: 7 X 7; stride 2
max pool: 3 X 3; stride 3
layer 1: 36 X 36 X 96
kernels: 7 X 7; stride 1
max pool: 2 X 2; stride 2
layer 2: 15 X 15 X 256
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 15 X 15 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 4: 15 X 15 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 5: 15 X 15 X 1024
padding: 1 all around
kernels: 3 X 3; stride 1
max pool: 3 X 3; stride 3
layer 6: 5 X 5 X 1024
FC:
layer 7: 1 X 1 X 4096
FC:
layer 8: 1 X 1 X 4096
op layer: 1000 way softmax

Compared to Alexnet (60 million parameters), overfeat has ~ 2X more (~140 million parameters). 
Applies to both the fast, accurate models.

Inference:
applied in a sliding window manner to 6 resolutions + their horizontal flip
Subsampling loss from accurate model is 2 (stride) X 3 (max-pool) X 2 (max-pool) X 3 (max-pool)
Can get rid of the last factor as follows:
apply layer 5 max-pool for all 9 (3X3) offsets of x, y. we obtain 9 versions of layer 5. 
apply rest of the net and obtain 9 versions of classification results (application for each version is in sliding manner, so 
the output will be a spatial classification map). interleave the 9 versions (e.g. x = 0 followed by x = 1, x = 2)
For each resolution + flip, each class's metric is taken as spatial max
the resulting 1000 dimensional vector is averaged across the resolutions and flips
the 6 resolutions have dimensions ranging from 245 to 569 and various aspect ratios

Results:
on ILSVRC 2012
Alexnet: top 5: 18.2
OverFeat fast model: single scale top 5: 16.97; six scales top 5: 16.27; 7 models 4 scales top 5: 13.8

Localization:
uses same feature extractor as classification but has a new regressor network on top of layer 5
layer 5:
FC:
layer 6: 1 X 1 X 4096
FC: 
layer 7: 1 X 1 X 1024
FC:
layer 8: 1 X 1 X 4 (coordinates) ( if done per class: i.e. 1 X 1 X 4000 )

Training:
fix the feature extraction layers (up to layer 5). Train the regressor network using L2 loss on true coordinates. However, do 
not train on bounding boxes that have less than 50% overlap on the field of view. Also, regressor is trained on the 6 scales.

Inference:
Similar to classification, multiple 3 X 3 offsets of layer 5 are evaluated.
Bounding box predictions are combined across scales and spatially to form a set of bounding box candidates. From this set, 
merge candidates are chosen as those who have smallest distance from their center to the center of their overlap. If this 
distance exceeds a threshold, merging stops. The candidates are merged by averaging their corner coordinates.
The class scores for the merged box is the sum of scores associated w/ input window corresponding to each bounding box that
participates in the merge.
per-class-regression (i.e. top layer per class) does worse than single-class-regression. likely due to lack of samples per class.

Detection:
not much detail. re-read.
did not retrain on the validation set. validation set distribution is significantly different enough from training set that it 
results in 1 pt better mAP.
came in 3rd but post-competition were able to improve to 1st. 

suggestions from paper:
backpropagate through all layers for localization. use IOU loss for localization (feasible as long as there is some overlap).
alternate parametrization of bounding box can decorrelate the output and result in better training. 


VGG - Very Deep Convolutional Networks for Large Scale Image Recognition

ILSVRC 2014: second in classification; first in localization

input is 224 X 224 pixels. 
per pixel mean value, across training set, is subtracted from each pixel
all convolutions as 'same' convolutions; i.e. padding of 1 all around for 3 X 3 kernels
LRN (alexnet) does not improve performance on ILSVRC; leads to increased computation and memory

Architecture:
Model E:
ip layer: 224 X 224 X 3
padding: 1 all around
kernels: 3 X 3; stride 1
layer 1: 224 X 224 X 64
layer 2: same as previous layer
max pool: 2 X 2; stride 2
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 112 X 112 X 128
layer 4: same as previous layer
max pool: 2 X 2; stride 2
layer 5: 56 X 56 X 256
layer 6: same as previous layer
layer 7: same as previous layer
layer 8: same as previous layer
max pool: 2 X 2; stride 2
layer 9: 28 X 28 X 512
layer 10: same as previous layer
layer 11: same as previous layer
layer 12: same as previous layer
max pool: 2 X 2; stride 2
layer 13: 14 X 14 X 512
layer 14: same as previous layer
layer 15: same as previous layer
layer 16: same as previous layer
max pool: 2 X 2; stride 2
FC:
layer 17: 1 X 1 X 4096
Dropout: 0.5
FC:
layer 18: 1 X 1 X 4096
Dropout: 0.5
FC:
layer 19: 1000 way softmax

total number of parameters is similar to OverFeat (~144 million)

with 3 X 3 kernels; three layers will reach an effective receptive field of 7 X 7 but will have fewer params than a single 
7 X 7 kernel. Also, it has three non-linearities vs. one.

Training:
mini-batch size of 256, momentum of 0.9, weight decay of 0.0005, learning rate initialized to 0.01 and dropped by 10X based 
on validation performance. 74 epochs were run.
despite larger number of parameters (v. Alexnet), it took fewer epochs to converge due to regularization imposed by use of 3 X 3
kernels instead of 11 X 11 etc and due to pre-initialization of certain layers.

Random crops were chosen from rescaled training images. At random, they were horizontally flipped and RGB shift (Alexnet) was 
carried out.

Fixed scale training at 256 and 384: Image is scaled such that smallest side is 256 pixels. Random crops are chosen and modifications made prior to training. Once trained; weights 
are copied over for training with smallest side set to 384 pixels. While training for 384, learning rate is decreased to 0.001
Multi scale training: Image is scaled such that smallest side is a random value between 256 and 512. First the network is 
trained for fixed scale of 384 pixels. Then all layers are fine tuned to adapt to the multi scale setting.

Initialization:
weights ~ N(0, 0.01)
first trained smaller model (A). then reused the layers from A to initialize layers in bigger model. The copied-over layers had
the same learning rate as other layers and hence were allowed to adapt. biases were set to 0 (why?). they later felt Glorot 
initialization would have been equally effective.


Test/Inference:
Image is rescaled so the smallest side is of certain dmension. Network is applied over the entire image resulting in a 
class score map of variable size. this is spatially averaged to get per class scores. similarly, the flipped image is evaluated
and class scores are avged between original and flipped versions.

Training done on 4 Nvidia Titan Black GPUs w/ speedup of 3.75. Data parallelism was achieved by each GPU computing over a 
a fraction of the mini-batch samples. The gradients computed by each GPU is averaged to obtain the full batch gradient. This is
done in a synchronous manner so the computations are the same as if training occured on one GPU.
Training a single net takes 2-3 weeks

Results:
error rate decreases with increasing depth, saturates at 19 layers. 
replacing pairs of 3 X 3 convolutions by a single 5 X 5 convolution layer results in 7% higher error rate.
scale jittering (i.e. randomly choosing a scale value between 256 and 512 and resizing to have the smaller side match that) 
leads to better performance than training on fixed scale value.
Model E with scale jittering achieves 8% top 5 error for single scale evaluation.

Multi-scale evaluation - run inference on several rescaled versions of image and average the resulting class probabilities. 
models trained with fixed size are evaluated with size around the fixed size. model trained with scale jittering are evaluated 
over wider variations in size since the training jitter is of wide variation.

perform comparably to the classification task winner (GoogLeNet) with delta of only 0.1%. GoogLeNet being 6.7% top 5 error.

Localization:
Won the localization challenge. Similar to OverFeat, can run either single-class-regression or per-class-regression. L2 loss is 
used in the regression layer.
Training done for fixed scales. Was initialized to the classification network trained for the same scale. Initial learning rate 
was 0.001. Explored fine-tuning all layers or only the fully connected layers.


Testing (Localization):
Dense application of the localization convent to produce class probabilities and bounding box proposals. Boxes were merged as in OverFeat. When multiple convnets are run, merging is done over the union of box proposals from the various convnets. Did not use the multiple pooling offsets technique of OverFeat which gives finer granularity.

Results:
Per class regression does ~2% better (i.e. error rate). Full fine-tuning does ~1% better than fine-tuning of the fully connected layers alone.
Applying the convnet densely over the whole image improves error by ~6%. Testing at multiple scales and combining predictions of multiple convnets is beneficial.
Results are better than OverFeat by ~5% despite not using as many scales and not running multiple pooling offsets. Thus, we can expect the improvement to have been due to use of better representations by means of deeper net.

Use as feature extractor:
Penultimate layer’s output is treated as image feature. Features are aggregated across locations and scales as follows: Similar to the classification task’s inference, the image is scaled so the smaller side is of certain dimension. The convent is then densely applied to get a 4096 channel spatial map (from the penultimate layer). This is averaged spatially to get a 4096 dimension descriptor. This descriptor is averaged with the descriptor for the horizontally flipped image. Similar descriptors are obtained for multiple scales and either stacked or pooled.
For running classification on other datasets (e.g. PASCAL VOC), the descriptor is L2 normalized and a linear SVM is trained (note: the weights of the convent remain unchanged). 
For PASCAL VOC, didn’t see any difference in stacking descriptors vs. averaging them across scales.
Achieves ~6% better performance on PASCAL VOC compared against others.

For caltech datasets, 3 random test / train splits were used. For each split, the number of images per class was controlled (e.g. for caltech 101, training set had 30 images per class while test set had 50 images per class). For caltech case, stacking the descriptors from different scales was better than pooling. Outperforms state of art by ~8%

On PASCAL VOC 2012 Action classification task; during training (of SVM), features are obtained in two ways - a) descriptor taken over entire image (ignoring the bounding box) b) descriptor taken over bounding box is stacked with the descriptor taken over entire image. Beats state of art by ~8%

VGG has been used in other areas as well - object detection, semantic segmentation, caption generation, texture and material recognition


————————————————————————————————
Andrew Ng Class notes
EM algorithm

Typically for ML estimation, we simply calculate the log likelihood (log P(x; theta)) summed over the independent samples (x1, x2, …) and optimize for theta. 
In case the model is affected by a latent variable, we will need to marginalize over the latent in order to estimate P(x; theta) and this may be intractable.

EM algorithm is applicable in this case. The strategy will be to construct a lower bound (E step) and tighten the bound (M step)

For each sample i, consider that the latent variable obeys a distribution Q_i(z)
log likelihood = sum( log P(x_i; theta) ) >= sum( Expectation_over_Q_i(z)[ log P(x_i, z; theta) / Q_i(z) ] )
for the jensen’s inequality to be tight, we need Q_i(z) proportional to P(x_i, z; theta) for all z; i.e. we need Q_i(z) to be P(z / x_i; theta).
Therefore; log likelihood = sum_over_i( sum_over_z( P(z / x_i; theta) log( P(x_i, z; theta) / P(z / x_i; theta) )))

Then to maximize the log likelihood, we optimize over theta. 
In EM algorithm, however, we run two steps

E-Step: Compute Q_i(z) = P(z / x_i; theta_1)
M-Step: Optimize theta_2 = argmax sum_over_i( sum_over_z ( Q_i(z) log( P(x_i, z; theta) / Q_i(z) )))
Iterate till convergence

Note: in the M-step, Q_i is based on theta_1 - i.e. it is held based on previous value of theta
Note 2: in the M-step, it may look like you are minimizing the KL divergence between Q_i(z) and P(x_i, z; theta)… but notice that with regard to z, P(x_i, z; theta) is not a probability distribution!
Note 3: so, instead of performing marginalization over z; we need to be able to compute P(z / x_i; theta)

elaborating on note 3: why does this matter practically speaking? consider ML estimation for mixture of Gaussian modeling. if you go with the marginalization approach, you will have weighted sum of  Gaussians inside the log - this prevents the exponential from getting cancelled out. Further, when you differentiate w.r.t the mean of the gaussian, you will have a ratio of exponentials as the derivative term and this prevents you from getting to a closed form solution. On the other hand, in the EM algorithm, in the M-step, we treat Q as some constant (having computed it in the E-step). thus the M-step is just like ML estimation where the latent variables are known. In the mixture of Gaussians case, it results in a closed form answer.


Why will the EM algorithm converge?
log likelihood with theta_2 >= sum_over_i( sum_over_z ( Q_i(z; theta_1) log( P(x_i, z; theta_2) / Q_i(z; theta_1) ) )) >= sum_over_i( sum_over_z( Q_i(z; theta_1) log( P(x_i, z; theta_1) / Q_i(z; theta_1) ))) = log likelihood with theta_1 (due to the way Q_i is chosen)

so, with each iteration, you are guaranteed to not get worse; and you are upper bounded by 0

further investigation: look into monte carlo EM (this is when monte carlo methods are applied in the E-step for computing Q(.) due to intractability)

——————————————

Fitting mixture of gaussians is an unsupervised problem?
---------------

auto-encoding variational bayes

See fig. 1 for the directed model discussed in the paper. 

There is a continuous latent variable z and an observed variable x. We want to do ML estimation of parameters. 
Assumption is that marginalizing P(x,z; theta) over z is intractable. Also, computing P(z / x; theta) is intractable (so EM algorithm doesn’t work). Integrals for any reasonable mean-field variational bayes algorithm are also intractable ???

provides solution to: a) estimating theta b) computing posterior P( z / x; theta ) c) efficient approximate marginal inference of x ??? ( i guess they mean generating x? )

Introduce an approximation to the true posterior P( z | x; theta) as Q( z | x; phi). terminology: Q( z | x; phi) is referred to as encoder (z is thought of as a code). P(x | z; theta) is referred to as a decoder.

----------------
Variational methods: Mean field approximation

Consider latent random variable z and visible random variable x. How do we compute the posterior P( z | x_i) or any function of it?
In variational inference, we use an approximating distribution Q( z | x_i; phi) to model the true posterior. This approximating
distribution is usually a simple one like a Gaussian.
(From mackay): The objective to be optimized is the variational free energy, given by :
 E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi) / P( z, x_i; theta ) ) )
equivalently, this is
 E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi) / P( z|x_i; theta ) ) ) - E_over_Q(z|x_i; phi)( log P(x_i; theta) )
= KL( Q(z|x_i; phi) || P( z|x_i; theta ) ) - log P(x_i; theta)
Note that the first term is the KL divergence which can be made zero by making Q(z|x_i; phi) converge to the true posterior.
Thus we have the following variational lower bound on the log likelihood
log P(x_i; theta) = KL( Q(z|x_i; phi) || P( z|x_i; theta ) ) - E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi)/ P(z, x_i; theta) ) )
>= - E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi)/ P(z, x_i; theta) ) ) (negative variational free energy / variational lower bound)

the variational bound can also be written in terms of P(z; theta) and P(x_i | z; theta) as 
KL( Q(z | x_i; phi) || P(z; theta) ) + E_over_Q(z | x_i; phi)( log(P(x_i | z; theta) ) - Here we use P(z) instead of P(x_i|z) in the 
KL divergence since P(x_i|z) is not a probability distribution over z.
The second term is called the expected reconstruction error. Why is this re-write of the bound useful? In some cases, we may be 
able to compute the first term analytically. Then sampling is necessary only for the expected reconstruction. The first term
can be interpreted as a regularizing effect, trying to keep the approximating distribution close to the prior on z.

So, the idea is to minimize the free energy
Note: the assumption is that it is difficult to compute/evaluate the true posterior P( z | x_i; theta ). 
However, it is possible to compute P(x_i | z; theta ), P(z; theta)

Now, we want to do gradient ascent to increase the value of the bound by modifying the variational parameters phi and the generative
parameters theta.

For computation of gradient of terms of the form E_over_Q(z | x_i; phi)( f(z) ); the typical MonteCarlo approach is to compute E_over_Q(z | x_i; phi)( f(z) Gradient_wrt_phi( log( Q(z | x_i; phi) ) ) ). This term has high variance ???why???
A reparametrization is done to work around this issue. Q(z | x_i; phi) is taken to be G_phi( e, x_i ) where G(.) is a deterministic function. e is the source of randomness following a distribution P( e ). E.g. P(e) can be N(0,1). G_phi(e, x_i) can be a linear transformation of e with scaling and offset determined by phi and x_i. Then Q(.) is a Gaussian. This reparametrization leads to easier differentiation w.r.t phi.

Recall the variational bound can be written in two forms:
1 : -E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi)/ P(z, x_i; theta) ) ) 

2: -KL( Q(z | x_i; phi) || P(z; theta) ) + E_over_Q(z | x_i; phi)( log(P(x_i | z; theta) ) 

With the reparametrization trick, these become

3: (1/L) Sum_1_to_L( log( P( z_i_l, x_i; theta ) ) - log( Q( z_i_l | x_i; phi ) ) ) where z_i_l is G_phi( e_l, x_i ) where e_l is sampled from P(e)
4: -KL( Q(z | x_i; phi) || P(z; theta) ) + (1/L) Sum_1_to_L( log( P(x_i | z_i_l; theta) ) ) where z_i_l is G_phi( e_l, x_i ) where e_l is sampled from P(e)

In equation 4, we assume the KL term simplifies into a closed form equation.
The second term of eq 4 has an auto-encoder interpretation. z is generated by sampling e from P(e) and applying G_phi( e, x_i ) for a given x_i. This z is then fed into a decoder to compute P( x_i | z; theta). This is called the reconstruction error in auto-encoder parlance.

Note: both EM and variational approaches have the same initial derivation…. in both cases, the log likelihood is lower bounded by negative of the variational free energy. In the EM case, we can compute the posterior of the latent variable, so this simplifies things.

Since the log likelihood of data x_i = KL( approx posterior || true posterior ) + variational lower bound; improving the variational lower bound will eventually improve the log likelihood of data because for the log likelihood to decrease despite the bound increasing, the KL term should decrease. However, the KL term is bounded below by zero, so it cannot decrease forever.

Variational auto encoder model: The prior P( z; theta ) is taken to be N(0, 1). P(x | z; theta) is taken to be a Gaussian whose parameters are computed by MLP. Note that the true posterior P(z | x; theta) is intractable (so EM algorithm cannot apply).  Q( z | x; phi) is also taken to be a Gaussian whose parameters are determined by another MLP. P( e ) is taken to be a multidimensional N(0, 1) Gaussian. 
e_l is sampled from P( e ); z_i_l is computed as G_phi(e_l, x_i) where the function G_phi is simply a scaling and offset applied to e_l. The parameters of the scaling and offset are determined by an MLP as a function of x_i.
Since the approximate posterior and the prior are both Gaussian, we can compute the KL term (and its derivative) in eq 2 exactly.
phi and theta are the corresponding MLP’s parameters.


Note that in eq 4 or 2; the second term is like the supervised learning objective - i.e. the corresponding MLP is trying to learn to map certain zs to certain type of xs
one MLP is learning to suggest certain Gaussian for each x_i (so, its sort of learning to discriminate? the generative MLP is learning to take samples from that Gaussian and map to the x_i

Results:
For training, a small weight decay term on theta was used. All weights were initialized from N(0, 0.01). Minibatch size was 100. Adagrad was used with step sizes of {0.01, 0.02, 0.1}. L = 1; i.e. only one sample of z is used for the reconstruction error estimate.

For MNIST, 500 hidden units were used; for Frey Faces, 200 was used. Dimension of latent variable was varied between 3 to 200 for MNIST; 2 to 20 for Frey Faces. Higher dimension did not overfit - likely due to the regularizing effect in eq 2/4.

Details of the MLP: consists of one hidden layer. from this hidden layer, you have one head estimating mu (mean of the Gaussian) and another estimating log(variance). For these, there is no non-linearity (since squashing would render the space of possibilities tiny). The activation function used for the hidden layer is tanh(.)

——————————
Tutorial on Variational Auto encoders

Additional notes on top of Auto encoding variational bases: One reason to have the generative process from z to x as probabilistic is that we can obtain gradients and learn. If it was deterministic
 learning would be difficult since in the deterministic case, the result either matches the ground truth or not and doesn’t provide indication of how to adjust the weights to make it match better.

Why is it good enough to use N(0,1) samples as source of randomness? the first few layers of the MLP can model a non-linear function to modify the distribution from N(0,1) into the appropriate distribution of the true latent factors (e.g for MNIST - slant, thickness, size etc.)

From scratch: Consider that you want to sample x from some distribution P( x ). we want P(.) to follow the empirical distribution on x. say we model P( x ) as being a function of a latent variable z (vector) - e.g. in the MNIST case, the latent variables would be the digit, slant, size etc. P( x | z ) is modeled by a MLP. In this case, we can assume z is N(0,1) and let the MLP use its first
few layers to go from N(0,1) to the true distribution over latent factors. the remaining layers will map the latent factors to the output in the following way: P( x | z ) = N(f(z; theta), sigma**2)
so, ideally we’ve got regions of the unit Gaussian z mapped to specific digits. How do we train this? we can run ML updates on theta by sampling z multiple times and taking average of P( x | z ) on these (i.e. monte carlo estimate of the E_over_z( P( x| z) ) = P(x) ). An issue with this is that for same/similar zs, we might at one run train it to map to digit a and another epoch train it to map to digit b and so on. We need some way of segregating z based on the digit we want it to map to - i.e. if we train a region of z to map to digit a, we should train another region of z to map to digit b.

Note that by having the regularization term in eq 2 and 4; we guarantee that the distributions of z that map to the various digits are close to a unit Gaussian. This is important during generation 
time because otherwise, we’d need to know the distribution of z for digit a in order to sample from it and thereby generate digit a. Instead, by keeping it close to unit Gaussian, we can just sample from the unit Gaussian.

In this paper, he claims that the main reason for reparametrization is that it allows the gradients computed on the reconstruction error term to flow back into the MLP responsible for the approximate posterior. This is true since the output of the MLP is mu and sigma terms (of the approximate posterior Q(.)) which connect to the generative MLP through addition and multiplication operations with the unit Gaussian noise e.
However, it is not pointed out that we can also have taken the gradient of the reconstruction error term w.r.t phi by E_over_Q(z | x_i; phi)( log(P(x_i|z; theta)) Gradient_over_phi( log(Q(z|x_i; phi)) ) ). To evaluate the Gradient_over_phi( log(Q(z|x_i; phi))), back propagation can be applied on the encoder (i.e. the one producing z) MLP. As per the Auto encoding variational bayes paper, this is not preferred because this manner of computing the gradient has large variance.

Conditional VAEs

Consider problems like hole filling where a part of an image has been cut out and it needs to be generated, or a string of digits is provided and we need to generate the next one. In these problems, the probability distribution of the feasible answers is multi-modal (e.g. only digits 0, 1, 2,. . . from the space of all images). If you run a regression on this where you are penalizing L2 norm between the prediction and ground truth, you’ll end up training the network to generate the centroid of the high probability points in the feasible space (e.g. an avg image over all digits). What is needed instead is a way of sampling from the multi modal distribution given an incomplete image or string of digits. Since the VAE enables us to sample from a multimodal distribution, we can extend it by conditioning on the incomplete image. Both the MLPs (encoder and decoder) take the incomplete image as an additional input. The approximate posterior is conditioned on the incomplete image and so is the generative probability. The prior on z is taken to be unit Gaussian, i.e. independent of the incomplete image. This is ok because the first few 
layers of the generative MLP will transform the distribution from unit Gaussian to the appropriate one for the latent variables and these layers can alter the result based on the incomplete image which is input.

Results:
MNIST VAE - run in Caffe using Relu and ADAM. For the generated image, each pixel is binarized and probability of being on is modeled with a sigmoid. performance is poor if z (latent) has very few dimensions (e.g. 4) or very large dimensions (e.g. 10000)
MNIST CVAE - conditioning is done on the middle column of pixels. Tried to show example where half the digit is provided and the CVAE completes the remaining half. This works, however a simple regression model also does this well. Likely this is due to lack of multi modal uncertainty given half the digit and/or due to overfitting.
------------------
Visualizing and Understanding Convolutional Networks

They show that transfer learning achieves new records on Caltech-101 and Caltech-256. The softmax layer alone is retrained.

Deconvnet is applied to generate the image corresponding to a particular activation. A layer is chosen and all but one neuron’s output is zero’ed out. The activations are then passed through the deconvnet.
Unpooling: While pooling, we note down the neurons which had the max value and had their activations transferred forward. For unpooling, this mapping is reversed. The ambiguity with this approach is that the non-max neurons’ values are unknown - just that they are less than the max’s.

Rectification: This is inverted by passing through relu again. Note that the activation values are non-negative. Thus, passing through relu to invert is reasonable.

Filtering: This is inverted by applying transposed versions of the filters - i.e the filters are flipped vertically and horizontally. Think of the neurons in the filtered layer being connected to the layer beneath. Filtering with the flipped filter is equivalent to sending the signal back through the same weighted connections.


Architecture:
relu activations throughout
ip layer: 224 X 224 X 3
kernels: 7 X 7; stride 2
intermediate layer: 110 X 110 X 96
max pool: 3 X 3; stride 2
contrast normalization across feature maps
layer 1: 55 X 55 X 96
kernels: 5 X 5; stride 2
intermediate layer: 26 X 26 X 256
max pool: 3 X 3; stride 2
contrast normalization across feature maps
layer 2: 13 X 13 X 256
kernels: 3 X 3; stride 1
layer 3: 13 X 13 X 384
kernels: 3 X 3; stride 1
layer 4: 13 X 13 X 384
kernels: 3 X 3; stride 1
intermediate layer: 13 X 13 X 256
max pool: 3 X 3; stride 2
layer 5: 6 X 6 X 256
FC:
layer 6: 4096
FC:
layer 7: 4096
FC: 
output layer: softmax over classes


Training:
Model was trained on ILSVRC 2012. Each image was rescaled so the smallest side is 256. center 256 X 256 is cropped. This is done for all images and the per pixel mean is subtracted. 10 224 X 224 crops are taken like in Alexnet (center, corners and flips). Mini batch size is 128, learning rate of 0.01
Momentum of 0.9. Learning rate is reduced manually by monitoring the validation error. Dropout is used in the fully connected layers w/ probability 0.5. Weights are initialized to 0.01 (note: not random??) and biases to 0. Trained for 70 epochs - took 12 days on GTX580.
They notice that in the first layer, some of the kernels dominate. These are normalized to have RMS (take root of the mean of the squared values) value clipped to 0.1 if it exceeds this.

Visualization:
For each layer, they look at the top 9 activations from a random subset of the feature maps. They deconv this back to image space and show the corresponding images. They also show the image patches corresponding to these top 9 activations. 
Note that since the deconvolution needs to remember the max switches, it can vary depending on the
image used as input.
By deconving from different layers over time, we see that the upper layers need 40-50 epochs to develop
Under image transformations such as translation, scaling and rotation, the lower layers are highly sensitive but sensitivity reduces as we go up the convnet. This is studied using the L2 loss between the feature maps of the modified image and the feature maps of the original image. The network is more sensitive to rotation than translation or scaling.
Alexnet visualizations show aliasing in layer 2 due to the large stride (4).

Results:
Removing the FC layers in alexnet results in ~4% higher error. Removing the top FC layer results in almost the same performance. Removing layers 3 and 4 also results in ~4% higher error.
Increasing the number of feature maps in the intermediate layers improves performance slightly (for their net, not done on Alexnet). In addition, if they double the number of neurons in the FC layers, they see overfitting.
To evaluate transfer learning, the top softmax layer is replaced and retrained. A linear SVM could be used instead. 
Some of the images overlap between ILSVRC 2012 and Caltech datasets. These were detected through normalized correlation and removed from ILSVRC during training of the convnet in order to avoid train/test contamination.
For caltech-101, picked 30 images per class for training (at random) and 50 images per class for testing. 5 train/test folds were used. per-class accuracies were reported. See reference for details.
Pre-training the convnet on ILSVRC, then applying on Caltech beats best reported result by 2% but starting with an untrained convnet does terribly (roughly half the accuracy)
For caltech-256, picked {15, 30, 45, 60} for training. Similar to caltech-101, pretrained beats the best while starting with untrained performs terribly. the pretrained model needs only 6 images per class to match leading methods using 60 images per class.
For PASCAL 2012, performance is close but doesn’t beat the best. PASCAL images contain multiple objects and may be of full scenes unlike ILSVRC images.
On caltech datasets, they experiment with retaining only the first x layers (adding a SVM or softmax on top) and see that depth provides advantage.

----------------------------------------
GoogLeNet

Driving motivations were to keep computational costs low while reaching higher depth and width. Uses 12X fewer parameters than Alexnet.
Won the ILSVRC 2014 classification challenge.
Budget target for inference time was 1.5 billion multiply adds.
Draws on ideas from ‘Network in Network’ paper.
Main issues with larger network are two fold: more labeled data necessary to train such (i.e. avoiding overfitting) and increased computation.
Inspired by Arora’s results (see reference) they claim a way to overcome these issues is to build a large network that has sparse connectivity. Arora’s result says that neurons that are highly correlated can be clustered (merged??). However, practically speaking, computation machinery is not as optimized for sparse computations as for dense ones like dense matrix multiplication. So, the inception architecture came out of a study into how the sparsity principles can be applied using dense computations. 

So, how can dense components be used to create local sparse structure for convolutional vision networks. < need to read through the references on network-in-network paper and the one by Arora to understand further >

Inception module: We’re not sure what convolution to run on a layer, so we run three types: 1x1, 3x3 and 5x5. Additionally, we also run 3x3 maxpooling. The next layer is simply a concatenation of these four types of feature maps. Note that the convolution layers are followed by relu.

Inception module w/ dimensionality reduction: In order to reduce the number of operations, we place 1x1 convolution layers (+relu) prior to the 3x3 and 5x5 layers in the Inception module. 1x1 conv layers (+relu) are also placed after the 3x3 maxpooling in order to reduce the number of feature maps.

An inception network is a stack of Inception modules. For memory reasons, the lower layers are kept as traditionally conv or pool layers. This is ok since anyway the lower layers only learn simple features. 

Architecture: See table 1

Input is 224 X 224 X 3 with per pixel mean subtraction.
Dropout 0.4 is applied in the fully connected layer.
In order to train the deep network (22 layers); auxiliary classification heads (avg pooling + conv + FC + softmax) were attached to 4a and 4d inception modules. The loss from these auxiliary heads was added to the main loss with a scaling of 0.3. Later experiments showed that you only needed one of the auxiliary heads and that too didn’t result in much of an improvement (less than 1%).

Training was done on distbelief framework with asynchronous SGD having momentum 0.9, fixed learning rate schedule (decrease by 4% every 8 epochs). Polyak averaging was used to create the final model.
For training, it is suggested to use various scales such that the area ranges from 8 to 100% of the original image area. Aspect ratio is suggested to be kept in the interval [3/4, 4/3]. Photometric distortions help reduce overfitting.

Testing used 7 independent models. 144 crops were taken per image: the image was rescaled so that the shorter side is of length {256, 288, 320, 352}. Then left, center, right (or top, center, bottom depending on which side is shorter) squares were extracted. From each square the four corner and center 224 x 224 crop is extracted. Also the square is resized to 224 x 224. Horizontal flips of the crops and resize are also used. 
The softmax output computed for each crop for each model is averaged over all models and crops.
1.5-2% better performance by using multiple crops. 1-2% better performance by using multiple models.

For the detection task, region proposals are done by combining selective search and multibox. 6 GoogLeNet models are used to identify the class in each region. Bounding box regression is not done. Despite this they rank 1st in detection
Since a good portion of localization dataset bounding boxes are not included in the detection dataset, the localization data may be used to pretrain the network’s bounding box regressor just like classification dataset is used to pretrain the classifier. GoogLeNet did not do this though.
—————————————————
Some improvements on deep convolutional neural network based image classification

———————
Deep Residual learning for Image recognition

If we increase the depth of typical convnets, we run into exploding/vanishing gradients problem and this has been addressed to a large extent by normalized initialization (like Glorot initialization) and by introducing batch normalization. Despite this a degradation problem arises wherein beyond a point, adding more layers makes training performance worse. 
Residual networks make use of layers that try to fit a residual mapping ( H(x) - x ) rather than the desired mapping H(x). 

Won ILSVRC 2015 classification, localization and detection. 

The resnet building block has at least two conv layers. The Relu after the second conv layer is performed only after adding x. If the building block only had one conv layer, it would be like a traditional conv layer with some of the weights fixed at 1. They do not observe any advantages in such case. If necessary, the shortcut connection can have a weight matrix multiplication (after flattening) in order to change the number of feature maps to match the output of the residual path (e.g. x is only 3 channels but the residual path outputs 10 feature maps). The addition of the residual and x is done per feature map.

The baseline convnet is inspired by VGGnet. Unlike VGG, it quickly reduces in resolution through use of a stride 2 convolution. For the remaining layers, it adds many more convolution layers at each resolution thereby bringing the total layers to 34.
The Resnet is obtained by adding skip connections to the 34 layer baseline convnet. A skip connection jumps over two convolution layers. At points where the skip connection goes from one resolution to another, the number of feature maps has doubled. To deal with the resolution decrease, either a) zero padding is used (i.e. the new feature maps are zero) or b) 1X1 convolution is performed on x. To match resolution, stride 2 is used.

Image is rescaled so the shorter side is in the interval [256, 480]. A random 224x224 crop is extracted from the rescaled image or its horizontal flip and per pixel mean (over training images) is subtracted. Color augmentation like in Alexnet (using PCA) is applied. Batch normalization is performed between convolution and activation throughout the network. Mini batch size 256, SGD with momentum 0.9 is applied. Learning rate is 0.1 with 10X reduction when the validation plateaus. weight decay is 0.0001. Dropout is not used since batch norm suffices. Trained for roughly 60x10e4 iterations (128 epochs). 
For testing, the network is densely applied and results averaged after rescaling so the shorter side is {224, 256, 384, 480, 640}.

For plain nets, degradation is observed despite use of batch normalization. i.e. 34 deep network has worse training error than 18 deep network. Resnets do not exhibit this issue. For the 18 deep network, it is observed that the resnet converges faster than plain net despite both achieving similar performance. Thus the shortcut connections help SGD. Performing 1x1 convolutions on the shortcut connections is better than zero padding as expected (since zero padding means those feature maps cannot do residual learning)

Bottlenecked building block: To ease computation, a bottlenecked design can be considered where instead of two 3x3 convolutions, we have three convolutions 1x1, 3x3 and 1x1 (the relu on the last one is applied only after adding in the short circuit connection - just like in the non-bottlenecked building block). the first 1x1 reduces the number of feature maps and the last 1x1 restores it back.
Having 1x1 convolutions on the short circuit path will dampen the benefit of bottlenecked building blocks due to the short circuit connecting high feature map points. Thus their use is to be minimized.

Object Detection: Faster R-CNN is used with ResNet-101 replacing VGG-16. Network is first trained on classification dataset, then fine tuned on detection dataset. Remaining TBD - needs background on R-CNN.

CIFAR 10 experiments: 32 x 32 images with per pixel mean subtracted are used. TBD

Note that there are no max pooling layers!

---------------------------------
Some improvements on Deep Convolutional Neural Network based Image Classification

In Alexnet, the image is rescaled so the smaller side is of length 256. Then the central 256x256 area is cropped out and 224x224 crops taken from it. This paper suggest to take random 224x224 crops from the full rescaled image of size 256XN (or NX256).
Python image library is used to modify brightness, contrast and color in the range of [0.5, 1.5]. After this, Alexnet style color modification (i.e. PCA) is done.
For testing, first three views are generated. Image is scaled such that the smaller side is 256. Three views are chosen (left, center, right or top, center, bottom). For each of these 256x256 images, three scales are generated (256, 228, 284 - bicubic interpolation is used for both up and down-scaling) and 5 crops taken from it (like Alexnet). Horizontal flips are also applied. This gives 90 predictions, which may be prohibitively large for real time use. 

Higher resolution model: if the crops are taken from the image scaled to a very different value, the convnet needs to be 
retrained for that new scale. Consider rescaling the image so the smaller side is 448. 224x224 crops are taken from this 
for training. In practice, instead what is done is that 128x128 patches are taken from the smaller-side-256 scaled images. 
128 is used since it is half of 256 (just like 224 is half of 448). The 128x128 patch is upscaled to 224x224 and used to train the convnet. The convnet is initialized to the lower resolution convnet and fine-tuning is done covering all layers. This completes in 30 epochs compared to the 90 needed for the low resolution case.
--------------------------------
RCNN

unlike classification, detection requires localizing many objects. At test time, ~2000 category independent region proposals are generated. Affine warping is used to rescale the region into a specific size and fed into a CNN. The CNN’s feature vector is fed to a category specific SVM to identify the object. Training data for detection is sparse, so supervised pre-training (on ILSVRC) is 
used resulting in 8 %-points improvement.
94% of the parameters of the network can be removed with minor drop in detection accuracy.
Analysis revealed that there were a large number of mislocalizations. Bounding box regression is added to counter this.

Selective Search is used to generate region proposals. For classification alone, a bit of context is added by increasing the region size such that post-warping, there are 16 pixels of context. 4096-dimensional vector is extracted by passing the rescaled region (227 x 227) through Alexnet. The input to Alexnet is mean-subtracted (mean values determined during training). The feature vector is taken from the last fully connected layer. The 4096 dimension feature is used to score each region based on a class-specific SVM. After scoring all regions, greedy non-maximum suppression is applied per class. This discards regions that have an IOU overlap w/ a higher scoring region more than a learned threshold. In practice all 2000 proposals are run through in a batch manner resulting in a 2000 x 4096 dimensional feature matrix. This is multiplied with 4096 x N matrix to compute the SVM product (N is the number of classes - recall SVM is class-specific).
To adapt the alexnet CNN to the warped images, it is fine-tuned (all layers) w/ the warped PASCAL VOC region proposals. The 1000 class softmax is replace by a 21 class version. Note that a background class is present. A region proposal is considered valid for a class
if IOU w/ ground truth exceeds 50%. Region proposals not belonging to any class are considered background. SGD is started with rate 10X lower than the initial rate used for pre-training. Each mini-batch is constructed to include 32 positive examples and 96 background classes. Post finetuning, SVM classifiers are trained per class. This follows a slightly different approach. For each SVM, positive examples are the ground truth bounding boxes. Negative examples are determined as those region proposals having less than 30% overlap with ground truth. This threshold of 30% was determined using grid search. Note that a region proposal may be negative for multiple classes. 
The reason for having a softmax for fine tuning and SVM for testing is affected by how the research exploration proceeded. Initially, the authors used the pre-trained Alexnet w/o fine-tuning. They determined the SVM training approach as described to be the optimal one. However, when they used the same approach for finetuning with softmax, they found that the number of positive examples was  not sufficient (since only ground truth is considered as positive). Thus they allowed for any region w/ IOU > 50% to be classified as positive (for finetuning). This expanded the positive set by 30X at the cost of precise localization. Another difference is that SVM training uses hard-negative mining while fine-tuning does not. Using the softmax classifier instead of SVM costs 4% points in mAP. Authors conjecture that better finetuning will remove the need for SVM classification.
For PASCAL VOC 2012, the CNN was finetuned on train and SVMs were optimized on trainval.
The learned features are visualized for each neuron by running a large set of held out samples, sorting them in decreasing activation strength and performing non-maximum suppression (i.e. discarding those that have IOU w/ a higher scoring sample of more than some threshold). The top samples for few neurons in layer 5 are shown in fig 3.
In ablation studies, they find that discarding both the fully connected layers doesn’t degrade performance by much despite getting rid of ~94% of the parameters. Discarding the top fully connected layer also doesn’t decrease performance by much (the top layer has 29% of the parameters). 
Finetuning boosts performance of the full convnet more than it boosts the performance of the convnet-w-both-fully-connected layers removed. This suggests that the features learned from ILSVRC by the non-fully connected layers are good enough for PASCAL VOC. It also suggests that the non-linear classifier needs to be adapted to the new domain (of PASCAL VOC).
Hoeim’s detection analysis tool is applied and identifies poor localization as a major source of error. Based on this, bounding box regression is added. Four functions are estimated as linear mappings of the pool5 features. The functions are correction terms for the x,y coordinates of the center of the region, its width and its height. The true x = P_w * correction_x + P_x, true y = P_h * correction_y + P_y, log( true w ) = log( P_w ) + correction_w, log(true h) = log(P_h) + correction_h; where P_i represents the current region proposal’s coordinates (x,y,w,h). The correction terms are learnt in a supervised manner using the ground truth to optimize least squares with weight decay (ridge regression). This regression learning is done only on proposals that have more than 60% overlap with a ground truth box, and is assigned to the class that has the highest overlap. Proposals with less than this amount of overlap are discarded. At test time, one could use the predicted bounding box to re-score the proposal (i.e. evaluate the CNN, SVM on the predicted region) and re predict the bounding box and iterate. However, authors report not seeing improved results from this approach.
Bounding box regression fixes a large number of mislocalization errors.

For semantic segmentation, CPMC (constrained parametric min-cuts) algorithm is applied to extract region proposals. Smallest enclosing window is extracted and warped to 227x227 pixels. Three strategies are tried: window is input as is, window is used with the non-selected pixels set to corresponding means ( so that they becomes zeroed out when passed into the CNN ); a concatenation of the features from the two approaches. The features are fed into SVR for regression (following the framework of O2P).

----------------------------
SPPNet

The approach to running CNNs on regions of an image are to warp the image after cropping and then to run on the warped image. Issues with this approach include: the cropped region may not contain the entire object, warping might heavily distort the object (e.g if the
region is a thin long one), post warping, the CNN loses some information on the scale of the object.
Considering CNNs to be made up of convolutional and fully connected layers, we see that it is only the fully connected layers that cannot deal with varying sized input. In CNNs, these fully connected layers appear at the highest layers. SPPNet introduces a SPP layer in between the convolution layers and the fully connected layers so that the convolutions can be done on the entire image (w/o need for cropping and warping) and still produce fixed size vectors for the fully connected layers to use.

SPPNet training is done with varying size images for scale invariance.
The SPP layer outputs a concatenation of vectors where each vector is formed by running maxpooling over a different sized grid (i.e. different kernel size and stride) applied to the output feature maps of the last convolution layer (note that they consider intermediate max pool layers as equivalent to convolution in the sense that both perform sliding). E.g. the first grid may contain only one max pooling operation running on the entire feature map; the second grid divides the feature map into four regions and max pooling is independently performed on each bin (for each feature map). the third grid divides the feature map into 16 regions and so on.

For training, note that most sw packages typically work with fixed size images (i.e. the size of the SPP kernels/strides may need to be known ahead of time).
Single size training: 224 x 224 crop of the image is taken and used in training. since the input size is fixed, we know apriori the kernel and stride sizes for each of the SPP layers.
Multi size training: this is done by using two fixed size networks that share parameters. one takes input of size 224 x 224 and the other 180 x 180. For generating 180 x 180 inputs, the 224 x 224 crop is simply resized instead of cropped again. During training, the 224 network is trained for a full epoch followed by 180 network for a full epoch and continued in this alternating fashion. Since they share weights, each is being updated every epoch.

Training is done on ILSVRC 2012 similar to alexnet: image is rescaled such that smaller side is 256. 11 224 x 224 crops are taken and color altering is performed (i.e. PCA based). Dropout is applied on the fully connected layers. 
Following grids are used for evaluation {6x6, 3x3, 2x2, 1x1}. With single-size trained network, performance is better by ~0.5%. With multi-size trained network, it is better by ~1%. In the multi-size scenario, training sizes are 180 and 224. Testing size is 224.
The paper also evaluates images in full view; i.e. the image is rescaled such that the min side is 256. Then the CNN is run on the entire rescaled image. This improves performance by ~0.2%. The feature vector from this operation may be used for similarity comparisons.
Multi-view testing on feature maps is performed by running the convolution layers on the entire image (image may be rescaled prior to doing so). Target windows on the image have corresponding windows on the feature maps that are output by the last conv layer. For each of these windows, SPP is performed followed by the fully connected layers and the softmax layer. The scores from multiple windows are averaged for the final prediction (e.g. 10  224x224 windows from a 256xN or Nx256 image)
How is a window in the image mapped to a window in the feature maps of the last conv layer? We just need to understand how a point is mapped from the image to feature map space: the target point is the one which has a receptive field with the center as close to the point in the image as possible. See appendix A for an example.
This idea is extended to extract multiple views from multiple scales for testing. Image is resized such that the smaller side is one of {224, 256, 300, 360, 448, 560}. From the rescaled image, 9 views are taken (center, corners and middle of each side). Horizontal flips are also taken. Additionally full image views are also used.

Their ILSVRC submission in classification was SPP built on Overfeat network.

They extend RCNN by making use of the SPP layer to avoid recomputing convolutions on each region proposal in the image. Convolutions are performed only once. Region proposals are mapped from the image space to the feature map space and SPP/fully-connected are run on the regions in the feature map space. In RCNN finetuning is done throughout the network to adapt it to the PASCAL dataset. In SPPNet, finetuning is done only on the fully connected layers (it will be complicated to backprop through the SPP layer)
Note though that SPPNet uses ZF-5 for detection.

For a pyramid level with n x n bins and a conv5 feature map size of a x a; the pooling window size is ceil(a/n) and stride is floor(a/n)


——————————————————————————

Fast RCNN

Uses VGG-16 and achieves significant speedup relative to RCNN and SPPnet. PASCAL VOC 2012 mAP increased from 62% for RCNN to 66% for Fast RCNN. 

Drawbacks of RCNN:
Multistage training pipeline: a) finetuning using softmax b) training SVM classifiers c) training bounding box regressor
Training takes a lot of time and space: 2) for SVM and bounding box regressor training, each region proposal of each image is warped  and passed through the network and the resulting features are written to disk. This takes couple of days in time and hundreds of gigabytes in space for the 5000 images in VOC2007 trainval set
Slow test time detection: VGG16 takes ~50s per image

Advantages of Fast RCNN:
better detection quality
single stage training
can train all layers (unlike SPPNet which only finetunes the fully connected layers)
no need to cache features

Each image and its regions of interest (RoI - region proposals) are fed through convolution layers (may include pooling) to obtain feature maps and the projection of the RoI on the feature maps. The projected RoI is pooled into a fixed size and passed through fully connected layers. Two heads are attached to the output of the fully connected layers: a new fully connected layer + a softmax layer for classification (classes + background) and a new fully connected layer + a regression layer for bounding box prediction (4 numbers for each class).
The RoI concept can be thought of as a simplified version of SPP wherein only one bin size is applied.

a convnet adopts Fast RCNN by doing the following 
a) the last max pool layer is replaced by the RoI pooling layer. the number of bins in the RoI is dependent on the input expected by the fully connected layer (e.g. 7x7 for VGG16).
b) the softmax classifier and the FC layer prior to it is replaced with two heads as described earlier (each having its own fully connected layer)
c) network is modified to take a batch of images and corresponding list of RoIs. The image resolution may change batch to batch

SPPNet and RCNN do RoI centric sampling for minibatch. i.e. each minibatch is a random collection of RoIs from all images, so each minibatch potentially contains multiple images. Fast RCNN does image-centric sampling where an image is randomly sampled and then RoIs are sampled from it. The advantage here is that RoIs from the same image can reuse the convolution layer’s computation. Thinking of it a bit differently, for each back prop iteration, the weights are updated, so forward prop needs to be updated too. This means that the conv layers need to recompute the feature maps for all the images in the mini batch. By sampling multiple RoIs from the same image, we reduce the number of distinct images per mini batch.

For the regression network, the parametrization is as per the equations used for RCNN. 
Loss function used is a combination of loss for the softmax head and the regression head. The softmax head follows the standard cross entropy loss. The regression head uses smoothed-L1 loss (quadratic for x < 1, linear for x > 1) on x, y, w, h and sums the four losses. The smoothed loss prevents exploding gradients when the estimation error is large.

Minibatch SGD: the minibatch is made of 128 RoIs, two images are randomly selected and 64 RoIs are randomly chosen from each. 25% of the RoIs are positive examples, i.e. have IoU > 50% with a ground truth class. Rest have < 50% overlap and are negative examples, i.e. assigned to the background class. RoIs are flipped horizontally w/ probability 0.5.
Backprop through RoI regions is as expected: Each minibatch consists of two images and 128 RoIs. Derivative w.r.t the pooled RoI neurons are computed as usual. Then, for each input neuron to the RoI pooling layer, its derivative is the sum over all RoIs of the derivative w.r.t certain neuron in the pooled RoI. The neuron in the pooled RoI is the one for which this neuron was the argmax - if such a situation exists. Alternately, one can think of the RoI pooling as selecting certain connections and disabling others. The derivatives backpropagate only through the connections that are selected.
The weights for the softmax are from N(0, 0.01); weights for the bounding box regressor are from N(0, 0.001). Learning rate for biases is 2X rate for weights. Global learning rate is 0.001. Momentum is 0.9 and weight decay is 0.0005.
Multiscale training is performed by randomly sampling a scale when an image is sampled (for the minibatch). During testing, multiscale is done by testing on an image pyramid. the list of RoIs provided to the network will indicate the level of the image pyramid to which the RoI is associated. Each predicted bounding box is associated with a confidence (i.e. the class probability). This is used in performing non-maximal suppression as in RCNN.

Speeding up inference in a fully connected layer: Consider weight W of a fully connected layer to be of dimension uxv. Decomposing through SVD to U*Sigma*V, U has dimensions uxt and V has dimensions txv. Sigma is of dimension txt. If the number of eigenvalues t is significantly less than min(u,v), we save parameters and computation because USigmaV needs ut+tv operations while W needs uv operations. The single W layer has been replaced by two fully connected layers, the first one with matrix Sigma*V (maps v dimensions to t dimensions) and the second one with U (maps t dimensions to u dimensions) and the original biases associated with W.  

On PASCAL VOC07, Fast RCNN mAP is similar to RCNN mAP. SPPNet is 3% lower - likely due to not finetuning all layers. On VOC10 and VOC12, Fast RCNN is 3-4% higher than RCNN.
Singificant speedup observed for both training and testing. 
SVD is used to replace the weights feeding into the FC6 and FC7 layers in VGG 16. In FC6 case, top 1024 singlular values are used out of the 25088x4096 matrix and in FC7 case, top 256 singular values are used out of the 4096x4096 matrix. 

Not fine-tuning the conv layers in VGG16 (i.e. layers below RoI pooling layer) results in 5% loss in VOC07. But this doesn’t mean we want to fine tune all conv layers. Finetuning from the third conv layer is sufficient for close to optimal performance. Including the 2nd conv layer adds 30% training time and including the 1st layer causes memory overrun.

Does multitask learning matter? On VOC07 mAP: Using the network trained with multitask loss but with bounding box regression turned off provides better performance than a network trained with the regression loss given zero weight. Also, two stage training where the network is first trained with regression loss given zero weight and then trained just on the regression loss (and the FC layer associated with it) with all other parameters frozen, does worse.

Does multiscale matter? Multiscale training and testing performs similarly to single scale training and testing.
Does SVM outperform Softmax? Recall that in RCNN we used SVMs for identifying the class. In Fast RCNN, softmax is used as the classifier. Training SVM on Fast RCNN with hard negative mining results in similar performance as the Softmax. 
In Fast RCNN, the sparse region proposals are the speed bottleneck. So, can we replace it with dense sliding window evaluation? Dense evaluation results in 5% reduction in mAP suggesting that the use of sparse proposals reduces regions that are false positives. If the detector is improved, then use of dense regions will be competitive since the detector is less likely to answer incorrectly.

————————————————
Faster RCNN

Region proposal algorithms are the bottleneck in detection. Selective search takes up to 2 sec per image and EdgeBoxes takes 0.2 seconds per image (about the same speed as the rest of the detection). Note though that these algorithms are run on cpu while the rest of the detection network is run on gpu. Even though running these on gpu is one way to speed them up, trying to do proposals through neural nets may be better since the net may share computation (i.e. layers) with the detection net.

Other papers have also proposed ways of using deep networks for predicting object bounding boxes. In Overfeat, box coordinates are predicted for each class by running the network in a sliding manner over the input. the box proposals are then merged.

Architecture of the RPN (Region Proposal Network): The network consists of convolution layers run in a sliding manner over the image to generate feature maps. E.g. the first 5 layers of ZF or first 13 layers of VGG 16 may be used for this purpose. Then a 3x3 convolution is done on these feature maps to output 256 dimension feature maps. Following this 1x1 convolution with 2*K dimension is performed for classification and a parallel 1x1 convolution with 4*K dimension is performed for bounding box prediction. Here K is the number of bounding box proposals made for each location of the 3x3 convolution window. The reason classification layer is 2*K is because for each proposal a 2 node softmax is used - to indicate whether the proposal contains an object or not. The box proposals are parametrized relative to anchors. anchors are boxes of predetermined size and scale centered at the center of the 3x3 window. There are 9 such anchors consisting of 3 scales and 3 aspect ratios. By using anchors of multiple scales, RPN avoids the need to run over images of various scales (as done in other tasks) or to use filters of various sizes.

Training: note that there are 9 anchors for each location of the 3x3 sliding window. Considering all the anchors over the image, some of them are assigned as containing objects, some as not containing objects and rest are ignored (i.e., not used for training). The positive
anchors are determined as those which have the maximum IoU with a ground truth box or an IoU > 70%. The negative anchors are those with IoU < 30% for any ground truth box. The loss function is a sum of classification loss (softmax loss for the two classes; summed over the anchor boxes) and regression loss (sum of smooth L1 applied on the error in the parametrized version of x, y, w, h - center x, center y, width and height) over all the positive/negative anchor boxes for the image. The regression loss term is present only for those anchor boxes that are positive.  The parametrization for the regression loss is like in RCNN but done w.r.t the anchor box (i.e. x = w_anchor*t+x_anchor, w = w_anchor*exp(t)). The ground truth box is parametrized to get t_ground_truth and the prediction is parametrized to get t_prediction. Smoothed L1 loss is taken on these parameters - because during inference t_prediction is what the network will be predicting and the ground truth of t_prediction is t_ground_truth.

Note that the features used for bounding box prediction are from the 3x3 sliding window. All 9 variants of the anchor boxes use the same feature from 3x3 sliding window. However the weights in the last layer (regression head) are different for each of the anchors. Contrast with RoI pooling (Fast RCNN) where the features are pooled from arbitrarily sized regions.

Mini batch sampling is image-centric like in Fast RCNN. Each minibatch is constructed from one image. positive and negative anchor boxes are randomly sampled to roughly get equal numbers. Note that if all positive/negative anchor boxes are used, the negative ones will dominate. New layers are initialized with N(0, 0.01). On VGG16, only layers from conv3 onward are fine tuned. Learning rate for fine tuning is 0.001 for 60k mini batches and 0.0001 for the next 20k mini batches on PASCAL VOC. Momentum is 0.9 and weight decay is 0.0005.

The full network consists of a combination of RPN and Fast RCNN. The convolution layers are shared between them. Following are various options for training the full network: a) Alternating training - train the RPN first, use its proposals to train Fast RCNN, refine the RPN, refine Fast RCNN and so on. Note that since the conv layers are shared, they benefit from updates in each step b) Approximate joint training - In this approach, the loss consists of the combination of the RPN and Fast RCNN losses. in the forward prop, the RPN proposals are used in Fast RCNN. In backward prop, the gradients from both networks’ losses are added up. Note that since the RoI pooling region is determined by the RPN network’s box predictions, we should consider the gradient of the Fast RCNN loss w.r.t these parameters as well. However the RoI pooling layer is not differentiable w.r.t these (e.g. think of RoI pooling over an interval [t_1, t_2] in single dimension. small changes in t_1 may not cause any change in the pooled layer - if it doesn’t change the pixels covered in the RoI or may cause a sudden change - if the newly covered pixel has a large value exceeding the current max).  c) non-Approximate training - use a differentiable RoI layer so that the gradient w.r.t the bounding box prediction parameters is not ignored

In the paper, a 4 step alternating training is used: first an RPN is trained. Then a Fast RCNN network is trained independently - not using the RPN’s weights. Then an RPN is initialized with the Fast RCNN’s weights and only the layers unique to RPN are fine-tuned. In the final step the layers unique to Fast RCNN are fine tuned. 

Both training and testing is done at single scale - the image is resized so that the smallest side is 600 pixels. The anchors are of areas {128**2, 256**2, 512**2} and aspect ratios {1:1, 1:2, 2:1}. Anchor boxes that cross the image boundary are ignored. including them prevents training convergence - may be due to all ground truth boxes being within the image, we don’t have enough samples for the case where the anchor boxes are outside? During testing, predictions crossing the image boundary are allowed - these are clipped to the image boundary.

Non maximum suppression is used to reduce the number of predictions. The RPN’s softmax scores are used for this purpose. IoU threshold for this is 70%. After NMS, the top proposals may be selected for use. Training uses 2000 proposals, testing uses 300, 1000, 2000.
Compared w/ Selective search and Edge boxes, the RPN provides competitive performance with just 300 proposals.
Changing the RPN’s convnet to VGG while keeping the Fast RCNN’s convnet as ZF improves mAP by 3%, suggesting that the region proposals can take advantage of better convnet architectures.

Using one anchor box instead of 9 reduces mAP by 3-4%. Using 3 scales alone or 3 aspect ratios alone performs almost as well as using all 9 anchor boxes.
Skipping the RPN layer and instead running Fast RCNN in a sliding window manner over the image, using 9 windows - 3 scales and 3 aspect ratios, results in ~5% lower mAP performance. This indicates that the objectness detection done by RPN is valuable to the Fast RCNN network.

 
—————————————————————————————
Polyak averaging:
average the parameters over past time steps; i.e. = (1/T) * sum_i_0toT parameter( i )
—————---------
How is the mini batch gradient computation implemented? is it that after forward prop, the gradients at the top are initialized (e.g. gradient of softmax w.r.t the logics) and back prop is done from that point on?
—————————————
what is mean average precision ??
——————————————————

what is hard negative mining ?

From ‘Object Detection with Discriminatively Trained Part Based Models’

In some cases such as detection, there may be a large number of negative examples (i.e. a large number of region proposals that do not contain the object). Training on all these is infeasible, hence we use hard negatives. They way this is done is that the detector is first trained with a reasonable set of negative examples. Then using this detector, hard negatives are collected. These are examples that the detector misclassifies as positives. A new model is trained with these hard negatives, and the process may be repeated a few times.

———————————————————————

</pre>
</body>
</html>
