Hello World!!

ILSVRC history:
Classification: Alexnet, Zeiler & Fergus, GoogLeNet (VGG), ResNet 
Localization: Alexnet, OverFeat, VGG, ResNet + RPN
Detection: ?, OverFeat, UvA-Euvision, GoogLeNet, ResNet + Faster-R-CNN

Datasets:
NORB
Caltech 101 / 256
CIFAR 10 / 100
MNIST
Imagenet - 15 million labeled images / 22000 categories
ILSVRC - 1.2 million labeled images, 50000 validation, 150000 testing. ~1000 images in each of 1000 categories
ILSVRC 2010 - test set labels are available

ILSVRC 2013 - localization considered correct only if > 50% overlap (intersection-over-union)
detection challenge may have zero objects (i.e. background class). background class makes detection harder.
localization and classification share data. detection has few more data points: images where the objects are smaller, images where
certain objects are absent (can be used for background class)
classification winner was clarifai ~11% (results in overfeat paper)
localization winner was overfeat 29.9%

ILSVRC 2012 and 2013 have the same train and test data? (mentioned in overfeat)

Preprocessing:


Regularization:
Data augmentation - adding noise, cropping, flipping, altering intensity of RGB images
Alexnet style RGB intensity alteration - collect all RGB values (3D vector) from all training images. PCA to compute eigenvectors 
and eigenvalues. Alter each training image by adding all three PCA directions scaled by their eigenvalue * N(0,0.1). Note: all 
pixels in image use the same gaussian sample but different images use different ones.
Dropout - reduces co-adaptation of features. increases the number of iterations needed to converge. e.g. w/ 0.5, alexnet needs
roughly 2X iterations (note: alexnet runs dropout only on two FC layers but these layers have most of the parameters)

Misc:
LRN - local response normalization - post Relu; normalize the activation by L2 norm of activations from 
neighboring kernels (n/2 on either side) raised to beta (hyperparameter). 
This ties the activations together, e.g. two activations can't both be high
Image similarity can be determined by L2 norm on the last hidden layer's representation of image. E.g. when used on ILSVRC, this 
retrieves similar images which in pixel-level-L2 would be far off.
Convolutions over entire image is more efficient than computing entire pipeline for each sliding window due to convolutions 
reusing overlapping computations (see fig 5 of Overfeat)

Optimization:
GD:
SGD:
Momentum:
Nesterov Momentum:
AdaGrad:
RMSProp:
Adam:


conv layer: for 1 based indexing: op dimension = floor( 1 + (ip dimension - kernel dimension + padding) / stride )

Title: ImageNet Classification with Deep Convolutional Neural Networks

Notes: 
60 million params, 650K neurons, dropout regularization, Relu
ILSVRC 2010 & 2012
1.2 million labeled images
5-6 days on 2 GTX580
rescaled img such that shorter side of image is 256. central 256x256 cropped out
subtracted the mean value, over training set, per pixel
LRN - local response normalization - reduces error rates by ~1.2%
Overlapping Pooling - makes it slightly harder to overfit

Architecture:
Relu activations throughout
ip layer: 224 X 224 X 3
kernels:  11 X 11 ; stride 4
layer 2:  55 X 55 X 96
LRN: neighborhood 5, beta 0.75, alpha, k
max pool: 3 x 3; stride 2
kernels:  5 X 5 ; stride 
layer 3:  27 X 27 X 256
LRN: same as above
max pool: 3 X 3; stride 2
kernels: 3 X 3 ; stride
layer 4: 13 X 13 X 384
kernels: 3 X 3 ; stride 
layer 5: 13 X 13 X 384
kernels: 3 X 3 ; stride
layer 6: 13 X 13 X 256
max pool: 3 X 3; stride 2
dropout: 0.5
layer 7: FC 4096
dropout: 0.5
layer 8: FC 4096
op layer: softmax 1000
Data augmentation - 2048 X increase in training set by cropping random 224x224 regions + their horizontal flips.
10 X during testing by considering the four corners, center + their horizontal flips
Learning: SGD, 0.9 momentum, mini-batch 128, 0.0005 weight decay. learning rate set to 0.01 and reduced 10X manually based on 
validation error. during training it went through 3 reductions. 
weight decay is not just regularizing. it helps reduce training error as well.
Initialization: weights are N(0,0.01). biases in layers 3, 5, 6 are set to 1; remaining are set to 0.
Training consumed 90 cycles of the entire training set (1.2 million images)

Results:
ILSVRC 2010: top 1: 37.5%; top 5: 17.0%; top 5 w/o avg-ing over 10 patches: 18.3%
ILSVRC 2012: top 5: 18.2%; 5 CNN avg top 5: 16.4%
Removing any of the conv layers results in 2% loss on top 1 performance.

OverFeat:
single convnet for classification, localization and detection.
avoid training on background class.
ILSVRC 2012 and 2013. Winner of 2013 localization.

Training:
Trained on ILSVRC 2012 (1.2 million images)
Image is rescaled so the smaller dimension is of 256 pixels. 
Extract 5 random crops (221 X 221) and feed them + their horizontal flips in mini-batches of 128.
weights initialized to N(0,0.01)
SGD with momentum 0.6, weight decay 0.00001
Learning rate: 0.05 and reduced 2X at (30, 50, 60, 70, 80) epochs

Architecture:
Training treats architecture as non-spatial (i.e. 1x1 output) but inference treats it as spatial.
Relu activations throughout
(typo in their paper on the first two layers of the fast model?)
Fast model:
ip layer: 221 X 221 X 3
kernels: 11 X 11; stride 4
max pool: 2 X 2; stride 2
layer 1: 28 X 28 X 96
kernels: 5 X 5; stride 1
max pool: 2 X 2; stride 2
layer 2: 12 X 12 X 256
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 12 X 12 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 4: 12 X 12 X 1024
padding: 1 all around
kernels: 3 X 3; stride 1
max pool: 2 X 2; stride 2
layer 5: 6 X 6 X 1024
FC:
layer 6: 1 X 1 X 3072
FC:
layer 7: 1 X 1 X 4096
softmax: 1000 classes

Accurate:
ip layer: 221 X 221 X 3
kernels: 7 X 7; stride 2
max pool: 3 X 3; stride 3
layer 1: 36 X 36 X 96
kernels: 7 X 7; stride 1
max pool: 2 X 2; stride 2
layer 2: 15 X 15 X 256
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 15 X 15 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 4: 15 X 15 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 5: 15 X 15 X 1024
padding: 1 all around
kernels: 3 X 3; stride 1
max pool: 3 X 3; stride 3
layer 6: 5 X 5 X 1024
FC:
layer 7: 1 X 1 X 4096
FC:
layer 8: 1 X 1 X 4096
op layer: 1000 way softmax

Compared to Alexnet (60 million parameters), overfeat has ~ 2X more (~140 million parameters). 
Applies to both the fast, accurate models.

Inference:
applied in a sliding window manner to 6 resolutions + their horizontal flip
Subsampling loss from accurate model is 2 (stride) X 3 (max-pool) X 2 (max-pool) X 3 (max-pool)
Can get rid of the last factor as follows:
apply layer 5 max-pool for all 9 (3X3) offsets of x, y. we obtain 9 versions of layer 5. 
apply rest of the net and obtain 9 versions of classification results (application for each version is in sliding manner, so 
the output will be a spatial classification map). interleave the 9 versions (e.g. x = 0 followed by x = 1, x = 2)
For each resolution + flip, each class's metric is taken as spatial max
the resulting 1000 dimensional vector is averaged across the resolutions and flips
the 6 resolutions have dimensions ranging from 245 to 569 and various aspect ratios

Results:
on ILSVRC 2012
Alexnet: top 5: 18.2
OverFeat fast model: single scale top 5: 16.97; six scales top 5: 16.27; 7 models 4 scales top 5: 13.8

Localization:
uses same feature extractor as classification but has a new regressor network on top of layer 5
layer 5:
FC:
layer 6: 1 X 1 X 4096
FC: 
layer 7: 1 X 1 X 1024
FC:
layer 8: 1 X 1 X 4 (coordinates) ( if done per class: i.e. 1 X 1 X 4000 )

Training:
fix the feature extraction layers (up to layer 5). Train the regressor network using L2 loss on true coordinates. However, do 
not train on bounding boxes that have less than 50% overlap on the field of view. Also, regressor is trained on the 6 scales.

Inference:
Similar to classification, multiple 3 X 3 offsets of layer 5 are evaluated.
Bounding box predictions are combined across scales and spatially to form a set of bounding box candidates. From this set, 
merge candidates are chosen as those who have smallest distance from their center to the center of their overlap. If this 
distance exceeds a threshold, merging stops. The candidates are merged by averaging their corner coordinates.
The class scores for the merged box is the sum of scores associated w/ input window corresponding to each bounding box that
participates in the merge.
per-class-regression (i.e. top layer per class) does worse than single-class-regression. likely due to lack of samples per class.

Detection:
not much detail. re-read.
did not retrain on the validation set. validation set distribution is significantly different enough from training set that it 
results in 1 pt better mAP.
came in 3rd but post-competition were able to improve to 1st. 

suggestions from paper:
backpropagate through all layers for localization. use IOU loss for localization (feasible as long as there is some overlap).
alternate parametrization of bounding box can decorrelate the output and result in better training. 


VGG - Very Deep Convolutional Networks for Large Scale Image Recognition

ILSVRC 2014: second in classification; first in localization

input is 224 X 224 pixels. 
per pixel mean value, across training set, is subtracted from each pixel
all convolutions as 'same' convolutions; i.e. padding of 1 all around for 3 X 3 kernels
LRN (alexnet) does not improve performance on ILSVRC; leads to increased computation and memory

Architecture:
Model E:
ip layer: 224 X 224 X 3
padding: 1 all around
kernels: 3 X 3; stride 1
layer 1: 224 X 224 X 64
layer 2: same as previous layer
max pool: 2 X 2; stride 2
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 112 X 112 X 128
layer 4: same as previous layer
max pool: 2 X 2; stride 2
layer 5: 56 X 56 X 256
layer 6: same as previous layer
layer 7: same as previous layer
layer 8: same as previous layer
max pool: 2 X 2; stride 2
layer 9: 28 X 28 X 512
layer 10: same as previous layer
layer 11: same as previous layer
layer 12: same as previous layer
max pool: 2 X 2; stride 2
layer 13: 14 X 14 X 512
layer 14: same as previous layer
layer 15: same as previous layer
layer 16: same as previous layer
max pool: 2 X 2; stride 2
FC:
layer 17: 1 X 1 X 4096
Dropout: 0.5
FC:
layer 18: 1 X 1 X 4096
Dropout: 0.5
FC:
layer 19: 1000 way softmax

total number of parameters is similar to OverFeat (~144 million)

with 3 X 3 kernels; three layers will reach an effective receptive field of 7 X 7 but will have fewer params than a single 
7 X 7 kernel. Also, it has three non-linearities vs. one.

Training:
mini-batch size of 256, momentum of 0.9, weight decay of 0.0005, learning rate initialized to 0.01 and dropped by 10X based 
on validation performance. 74 epochs were run.
despite larger number of parameters (v. Alexnet), it took fewer epochs to converge due to regularization imposed by use of 3 X 3
kernels instead of 11 X 11 etc and due to pre-initialization of certain layers.

Random crops were chosen from rescaled training images. At random, they were horizontally flipped and RGB shift (Alexnet) was 
carried out.

Fixed scale training at 256 and 384: Image is scaled such that smallest side is 256 pixels. Random crops are chosen and modifications made prior to training. Once trained; weights 
are copied over for training with smallest side set to 384 pixels. While training for 384, learning rate is decreased to 0.001
Multi scale training: Image is scaled such that smallest side is a random value between 256 and 512. First the network is 
trained for fixed scale of 384 pixels. Then all layers are fine tuned to adapt to the multi scale setting.

Initialization:
weights ~ N(0, 0.01)
first trained smaller model (A). then reused the layers from A to initialize layers in bigger model. The copied-over layers had
the same learning rate as other layers and hence were allowed to adapt. biases were set to 0 (why?). they later felt Glorot 
initialization would have been equally effective.


Test/Inference:
Image is rescaled so the smallest side is of certain dmension. Network is applied over the entire image resulting in a 
class score map of variable size. this is spatially averaged to get per class scores. similarly, the flipped image is evaluated
and class scores are avged between original and flipped versions.

Training done on 4 Nvidia Titan Black GPUs w/ speedup of 3.75. Data parallelism was achieved by each GPU computing over a 
a fraction of the mini-batch samples. The gradients computed by each GPU is averaged to obtain the full batch gradient. This is
done in a synchronous manner so the computations are the same as if training occured on one GPU.
Training a single net takes 2-3 weeks

Results:
error rate decreases with increasing depth, saturates at 19 layers. 
replacing pairs of 3 X 3 convolutions by a single 5 X 5 convolution layer results in 7% higher error rate.
scale jittering (i.e. randomly choosing a scale value between 256 and 512 and resizing to have the smaller side match that) 
leads to better performance than training on fixed scale value.
Model E with scale jittering achieves 8% top 5 error for single scale evaluation.

Multi-scale evaluation - run inference on several rescaled versions of image and average the resulting class probabilities. 
models trained with fixed size are evaluated with size around the fixed size. model trained with scale jittering are evaluated 
over wider variations in size since the training jitter is of wide variation.

perform comparably to the classification task winner (GoogLeNet) with delta of only 0.1%. GoogLeNet being 6.7% top 5 error.

Localization:

