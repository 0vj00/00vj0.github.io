Hello World!!

ILSVRC history:
Classification: Alexnet, Zeiler & Fergus, GoogLeNet (VGG), ResNet 
Localization: Alexnet, OverFeat, VGG, ResNet + RPN
Detection: ?, OverFeat, UvA-Euvision, GoogLeNet, ResNet + Faster-R-CNN

Datasets:
NORB
Caltech 101 / 256
CIFAR 10 / 100
MNIST
Imagenet - 15 million labeled images / 22000 categories
ILSVRC - 1.2 million labeled images, 50000 validation, 150000 testing. ~1000 images in each of 1000 categories
ILSVRC 2010 - test set labels are available

ILSVRC 2013 - localization considered correct only if > 50% overlap (intersection-over-union). For localization, a single box is
predicted for the predicted class.
detection challenge may have zero objects (i.e. background class). background class makes detection harder.
localization and classification share data. detection has few more data points: images where the objects are smaller, images where
certain objects are absent (can be used for background class)
classification winner was clarifai ~11% (results in overfeat paper)
localization winner was overfeat 29.9%

ILSVRC 2012 and 2013 have the same train and test data? (mentioned in overfeat)

PASCAL VOC 2007 - ~10K images for classification task; 20 object categories; mAP metric is used
PASCAL VOC 2012 - ~22K images for classification task; 20 object categories; mAP metric is used
Caltech 101 - 9K images in 102 categories (101 + a background class)
Caltech 256 - 31K images in 257 categories
PASCAL VOC 2012 Action classification - predict action given a bounding box around a person performing an action. 4.6K images and 11 classes. mAP metric is used.

The way caltech dataset is typically evaluated: Generate random train/test splits and report average performance across them. Mean class recall is used since it compensates for different number of test images per class.


Preprocessing:


Regularization:
Data augmentation - adding noise, cropping, flipping, altering intensity of RGB images
Alexnet style RGB intensity alteration - collect all RGB values (3D vector) from all training images. PCA to compute eigenvectors 
and eigenvalues. Alter each training image by adding all three PCA directions scaled by their eigenvalue * N(0,0.1). Note: all 
pixels in image use the same gaussian sample but different images use different ones.
Dropout - reduces co-adaptation of features. increases the number of iterations needed to converge. e.g. w/ 0.5, alexnet needs
roughly 2X iterations (note: alexnet runs dropout only on two FC layers but these layers have most of the parameters)

Misc:
LRN - local response normalization - post Relu; normalize the activation by L2 norm of activations from 
neighboring kernels (n/2 on either side) raised to beta (hyperparameter). 
This ties the activations together, e.g. two activations can't both be high
Image similarity can be determined by L2 norm on the last hidden layer's representation of image. E.g. when used on ILSVRC, this 
retrieves similar images which in pixel-level-L2 would be far off.
Convolutions over entire image is more efficient than computing entire pipeline for each sliding window due to convolutions 
reusing overlapping computations (see fig 5 of Overfeat)

Optimization:
GD:
SGD:
Momentum:
Nesterov Momentum:
AdaGrad:
RMSProp:
Adam:


conv layer: for 1 based indexing: op dimension = floor( 1 + (ip dimension - kernel dimension + padding) / stride )

Title: ImageNet Classification with Deep Convolutional Neural Networks

Notes: 
60 million params, 650K neurons, dropout regularization, Relu
ILSVRC 2010 & 2012
1.2 million labeled images
5-6 days on 2 GTX580
rescaled img such that shorter side of image is 256. central 256x256 cropped out
subtracted the mean value, over training set, per pixel
LRN - local response normalization - reduces error rates by ~1.2%
Overlapping Pooling - makes it slightly harder to overfit

Architecture:
Relu activations throughout
ip layer: 224 X 224 X 3
kernels:  11 X 11 ; stride 4
layer 2:  55 X 55 X 96
LRN: neighborhood 5, beta 0.75, alpha, k
max pool: 3 x 3; stride 2
kernels:  5 X 5 ; stride 
layer 3:  27 X 27 X 256
LRN: same as above
max pool: 3 X 3; stride 2
kernels: 3 X 3 ; stride
layer 4: 13 X 13 X 384
kernels: 3 X 3 ; stride 
layer 5: 13 X 13 X 384
kernels: 3 X 3 ; stride
layer 6: 13 X 13 X 256
max pool: 3 X 3; stride 2
dropout: 0.5
layer 7: FC 4096
dropout: 0.5
layer 8: FC 4096
op layer: softmax 1000
Data augmentation - 2048 X increase in training set by cropping random 224x224 regions + their horizontal flips.
10 X during testing by considering the four corners, center + their horizontal flips
Learning: SGD, 0.9 momentum, mini-batch 128, 0.0005 weight decay. learning rate set to 0.01 and reduced 10X manually based on 
validation error. during training it went through 3 reductions. 
weight decay is not just regularizing. it helps reduce training error as well.
Initialization: weights are N(0,0.01). biases in layers 3, 5, 6 are set to 1; remaining are set to 0.
Training consumed 90 cycles of the entire training set (1.2 million images)

Results:
ILSVRC 2010: top 1: 37.5%; top 5: 17.0%; top 5 w/o avg-ing over 10 patches: 18.3%
ILSVRC 2012: top 5: 18.2%; 5 CNN avg top 5: 16.4%
Removing any of the conv layers results in 2% loss on top 1 performance.

OverFeat:
single convnet for classification, localization and detection.
avoid training on background class.
ILSVRC 2012 and 2013. Winner of 2013 localization.

Training:
Trained on ILSVRC 2012 (1.2 million images)
Image is rescaled so the smaller dimension is of 256 pixels. 
Extract 5 random crops (221 X 221) and feed them + their horizontal flips in mini-batches of 128.
weights initialized to N(0,0.01)
SGD with momentum 0.6, weight decay 0.00001
Learning rate: 0.05 and reduced 2X at (30, 50, 60, 70, 80) epochs

Architecture:
Training treats architecture as non-spatial (i.e. 1x1 output) but inference treats it as spatial.
Relu activations throughout
(typo in their paper on the first two layers of the fast model?)
Fast model:
ip layer: 221 X 221 X 3
kernels: 11 X 11; stride 4
max pool: 2 X 2; stride 2
layer 1: 28 X 28 X 96
kernels: 5 X 5; stride 1
max pool: 2 X 2; stride 2
layer 2: 12 X 12 X 256
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 12 X 12 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 4: 12 X 12 X 1024
padding: 1 all around
kernels: 3 X 3; stride 1
max pool: 2 X 2; stride 2
layer 5: 6 X 6 X 1024
FC:
layer 6: 1 X 1 X 3072
FC:
layer 7: 1 X 1 X 4096
softmax: 1000 classes

Accurate:
ip layer: 221 X 221 X 3
kernels: 7 X 7; stride 2
max pool: 3 X 3; stride 3
layer 1: 36 X 36 X 96
kernels: 7 X 7; stride 1
max pool: 2 X 2; stride 2
layer 2: 15 X 15 X 256
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 15 X 15 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 4: 15 X 15 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 5: 15 X 15 X 1024
padding: 1 all around
kernels: 3 X 3; stride 1
max pool: 3 X 3; stride 3
layer 6: 5 X 5 X 1024
FC:
layer 7: 1 X 1 X 4096
FC:
layer 8: 1 X 1 X 4096
op layer: 1000 way softmax

Compared to Alexnet (60 million parameters), overfeat has ~ 2X more (~140 million parameters). 
Applies to both the fast, accurate models.

Inference:
applied in a sliding window manner to 6 resolutions + their horizontal flip
Subsampling loss from accurate model is 2 (stride) X 3 (max-pool) X 2 (max-pool) X 3 (max-pool)
Can get rid of the last factor as follows:
apply layer 5 max-pool for all 9 (3X3) offsets of x, y. we obtain 9 versions of layer 5. 
apply rest of the net and obtain 9 versions of classification results (application for each version is in sliding manner, so 
the output will be a spatial classification map). interleave the 9 versions (e.g. x = 0 followed by x = 1, x = 2)
For each resolution + flip, each class's metric is taken as spatial max
the resulting 1000 dimensional vector is averaged across the resolutions and flips
the 6 resolutions have dimensions ranging from 245 to 569 and various aspect ratios

Results:
on ILSVRC 2012
Alexnet: top 5: 18.2
OverFeat fast model: single scale top 5: 16.97; six scales top 5: 16.27; 7 models 4 scales top 5: 13.8

Localization:
uses same feature extractor as classification but has a new regressor network on top of layer 5
layer 5:
FC:
layer 6: 1 X 1 X 4096
FC: 
layer 7: 1 X 1 X 1024
FC:
layer 8: 1 X 1 X 4 (coordinates) ( if done per class: i.e. 1 X 1 X 4000 )

Training:
fix the feature extraction layers (up to layer 5). Train the regressor network using L2 loss on true coordinates. However, do 
not train on bounding boxes that have less than 50% overlap on the field of view. Also, regressor is trained on the 6 scales.

Inference:
Similar to classification, multiple 3 X 3 offsets of layer 5 are evaluated.
Bounding box predictions are combined across scales and spatially to form a set of bounding box candidates. From this set, 
merge candidates are chosen as those who have smallest distance from their center to the center of their overlap. If this 
distance exceeds a threshold, merging stops. The candidates are merged by averaging their corner coordinates.
The class scores for the merged box is the sum of scores associated w/ input window corresponding to each bounding box that
participates in the merge.
per-class-regression (i.e. top layer per class) does worse than single-class-regression. likely due to lack of samples per class.

Detection:
not much detail. re-read.
did not retrain on the validation set. validation set distribution is significantly different enough from training set that it 
results in 1 pt better mAP.
came in 3rd but post-competition were able to improve to 1st. 

suggestions from paper:
backpropagate through all layers for localization. use IOU loss for localization (feasible as long as there is some overlap).
alternate parametrization of bounding box can decorrelate the output and result in better training. 


VGG - Very Deep Convolutional Networks for Large Scale Image Recognition

ILSVRC 2014: second in classification; first in localization

input is 224 X 224 pixels. 
per pixel mean value, across training set, is subtracted from each pixel
all convolutions as 'same' convolutions; i.e. padding of 1 all around for 3 X 3 kernels
LRN (alexnet) does not improve performance on ILSVRC; leads to increased computation and memory

Architecture:
Model E:
ip layer: 224 X 224 X 3
padding: 1 all around
kernels: 3 X 3; stride 1
layer 1: 224 X 224 X 64
layer 2: same as previous layer
max pool: 2 X 2; stride 2
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 112 X 112 X 128
layer 4: same as previous layer
max pool: 2 X 2; stride 2
layer 5: 56 X 56 X 256
layer 6: same as previous layer
layer 7: same as previous layer
layer 8: same as previous layer
max pool: 2 X 2; stride 2
layer 9: 28 X 28 X 512
layer 10: same as previous layer
layer 11: same as previous layer
layer 12: same as previous layer
max pool: 2 X 2; stride 2
layer 13: 14 X 14 X 512
layer 14: same as previous layer
layer 15: same as previous layer
layer 16: same as previous layer
max pool: 2 X 2; stride 2
FC:
layer 17: 1 X 1 X 4096
Dropout: 0.5
FC:
layer 18: 1 X 1 X 4096
Dropout: 0.5
FC:
layer 19: 1000 way softmax

total number of parameters is similar to OverFeat (~144 million)

with 3 X 3 kernels; three layers will reach an effective receptive field of 7 X 7 but will have fewer params than a single 
7 X 7 kernel. Also, it has three non-linearities vs. one.

Training:
mini-batch size of 256, momentum of 0.9, weight decay of 0.0005, learning rate initialized to 0.01 and dropped by 10X based 
on validation performance. 74 epochs were run.
despite larger number of parameters (v. Alexnet), it took fewer epochs to converge due to regularization imposed by use of 3 X 3
kernels instead of 11 X 11 etc and due to pre-initialization of certain layers.

Random crops were chosen from rescaled training images. At random, they were horizontally flipped and RGB shift (Alexnet) was 
carried out.

Fixed scale training at 256 and 384: Image is scaled such that smallest side is 256 pixels. Random crops are chosen and modifications made prior to training. Once trained; weights 
are copied over for training with smallest side set to 384 pixels. While training for 384, learning rate is decreased to 0.001
Multi scale training: Image is scaled such that smallest side is a random value between 256 and 512. First the network is 
trained for fixed scale of 384 pixels. Then all layers are fine tuned to adapt to the multi scale setting.

Initialization:
weights ~ N(0, 0.01)
first trained smaller model (A). then reused the layers from A to initialize layers in bigger model. The copied-over layers had
the same learning rate as other layers and hence were allowed to adapt. biases were set to 0 (why?). they later felt Glorot 
initialization would have been equally effective.


Test/Inference:
Image is rescaled so the smallest side is of certain dmension. Network is applied over the entire image resulting in a 
class score map of variable size. this is spatially averaged to get per class scores. similarly, the flipped image is evaluated
and class scores are avged between original and flipped versions.

Training done on 4 Nvidia Titan Black GPUs w/ speedup of 3.75. Data parallelism was achieved by each GPU computing over a 
a fraction of the mini-batch samples. The gradients computed by each GPU is averaged to obtain the full batch gradient. This is
done in a synchronous manner so the computations are the same as if training occured on one GPU.
Training a single net takes 2-3 weeks

Results:
error rate decreases with increasing depth, saturates at 19 layers. 
replacing pairs of 3 X 3 convolutions by a single 5 X 5 convolution layer results in 7% higher error rate.
scale jittering (i.e. randomly choosing a scale value between 256 and 512 and resizing to have the smaller side match that) 
leads to better performance than training on fixed scale value.
Model E with scale jittering achieves 8% top 5 error for single scale evaluation.

Multi-scale evaluation - run inference on several rescaled versions of image and average the resulting class probabilities. 
models trained with fixed size are evaluated with size around the fixed size. model trained with scale jittering are evaluated 
over wider variations in size since the training jitter is of wide variation.

perform comparably to the classification task winner (GoogLeNet) with delta of only 0.1%. GoogLeNet being 6.7% top 5 error.

Localization:
Won the localization challenge. Similar to OverFeat, can run either single-class-regression or per-class-regression. L2 loss is 
used in the regression layer.
Training done for fixed scales. Was initialized to the classification network trained for the same scale. Initial learning rate 
was 0.001. Explored fine-tuning all layers or only the fully connected layers.


Testing (Localization):
Dense application of the localization convent to produce class probabilities and bounding box proposals. Boxes were merged as in OverFeat. When multiple convnets are run, merging is done over the union of box proposals from the various convnets. Did not use the multiple pooling offsets technique of OverFeat which gives finer granularity.

Results:
Per class regression does ~2% better (i.e. error rate). Full fine-tuning does ~1% better than fine-tuning of the fully connected layers alone.
Applying the convnet densely over the whole image improves error by ~6%. Testing at multiple scales and combining predictions of multiple convnets is beneficial.
Results are better than OverFeat by ~5% despite not using as many scales and not running multiple pooling offsets. Thus, we can expect the improvement to have been due to use of better representations by means of deeper net.

Use as feature extractor:
Penultimate layer’s output is treated as image feature. Features are aggregated across locations and scales as follows: Similar to the classification task’s inference, the image is scaled so the smaller side is of certain dimension. The convent is then densely applied to get a 4096 channel spatial map (from the penultimate layer). This is averaged spatially to get a 4096 dimension descriptor. This descriptor is averaged with the descriptor for the horizontally flipped image. Similar descriptors are obtained for multiple scales and either stacked or pooled.
For running classification on other datasets (e.g. PASCAL VOC), the descriptor is L2 normalized and a linear SVM is trained (note: the weights of the convent remain unchanged). 
For PASCAL VOC, didn’t see any difference in stacking descriptors vs. averaging them across scales.
Achieves ~6% better performance on PASCAL VOC compared against others.

For caltech datasets, 3 random test / train splits were used. For each split, the number of images per class was controlled (e.g. for caltech 101, training set had 30 images per class while test set had 50 images per class). For caltech case, stacking the descriptors from different scales was better than pooling. Outperforms state of art by ~8%

On PASCAL VOC 2012 Action classification task; during training (of SVM), features are obtained in two ways - a) descriptor taken over entire image (ignoring the bounding box) b) descriptor taken over bounding box is stacked with the descriptor taken over entire image. Beats state of art by ~8%

VGG has been used in other areas as well - object detection, semantic segmentation, caption generation, texture and material recognition


————————————————————————————————
auto-encoding variational bayes

See fig. 1 for the directed model discussed in the paper. 

There is a continuous latent variable z and an observed variable x. We want to do ML estimation of parameters. 
Assumption is that marginalizing P(x,z; theta) over z is intractable. Also, computing P(z / x; theta) is intractable (so EM algorithm doesn’t work). Integrals for any reasonable mean-field variational bayes algorithm are also intractable ???

provides solution to: a) estimating theta b) computing posterior P( z / x; theta ) c) efficient approximate marginal inference of x ??? ( i guess they mean generating x? )

Introduce an approximation to the true posterior P( z | x; theta) as Q( z | x; phi). terminology: Q( z | x; phi) is referred to as encoder (z is thought of as a code). P(x | z; theta) is referred to as a decoder.

————————
Andrew Ng Class notes
EM algorithm

Typically for ML estimation, we simply calculate the log likelihood (log P(x; theta)) summed over the independent samples (x1, x2, …) and optimize for theta. 
In case the model is affected by a latent variable, we will need to marginalize over the latent in order to estimate P(x; theta) and this may be intractable.

EM algorithm is applicable in this case. The strategy will be to construct a lower bound (E step) and tighten the bound (M step)

For each sample i, consider that the latent variable obeys a distribution Q_i(z)
log likelihood = sum( log P(x_i; theta) ) >= sum( Expectation_over_Q_i(z)[ log P(x_i, z; theta) / Q_i(z) ] )
for the jensen’s inequality to be tight, we need Q_i(z) proportional to P(x_i, z; theta) for all z; i.e. we need Q_i(z) to be P(z / x_i; theta).
Therefore; log likelihood = sum_over_i( sum_over_z( P(z / x_i; theta) log( P(x_i, z; theta) / P(z / x_i; theta) )))

Then to maximize the log likelihood, we optimize over theta. 
In EM algorithm, however, we run two steps

E-Step: Compute Q_i(z) = P(z / x_i; theta_1)
M-Step: Optimize theta_2 = argmax sum_over_i( sum_over_z ( Q_i(z) log( P(x_i, z; theta) / Q_i(z) )))
Iterate till convergence

Note: in the M-step, Q_i is based on theta_1 - i.e. it is held based on previous value of theta
Note 2: in the M-step, it may look like you are minimizing the KL divergence between Q_i(z) and P(x_i, z; theta)… but notice that with regard to z, P(x_i, z; theta) is not a probability distribution!
Note 3: so, instead of performing marginalization over z; we need to be able to compute P(z / x_i; theta)

elaborating on note 3: why does this matter practically speaking? consider ML estimation for mixture of Gaussian modeling. if you go with the marginalization approach, you will have weighted sum of  Gaussians inside the log - this prevents the exponential from getting cancelled out. Further, when you differentiate w.r.t the mean of the gaussian, you will have a ratio of exponentials as the derivative term and this prevents you from getting to a closed form solution. On the other hand, in the EM algorithm, in the M-step, we treat Q as some constant (having computed it in the E-step). thus the M-step is just like ML estimation where the latent variables are known. In the mixture of Gaussians case, it results in a closed form answer.


Why will the EM algorithm converge?
log likelihood with theta_2 >= sum_over_i( sum_over_z ( Q_i(z; theta_1) log( P(x_i, z; theta_2) / Q_i(z; theta_1) ) )) >= sum_over_i( sum_over_z( Q_i(z; theta_1) log( P(x_i, z; theta_1) / Q_i(z; theta_1) ))) = log likelihood with theta_1 (due to the way Q_i is chosen)

so, with each iteration, you are guaranteed to not get worse; and you are upper bounded by 0

further investigation: look into monte carlo EM (this is when monte carlo methods are applied in the E-step for computing Q(.) due to intractability)

——————————————

Fitting mixture of gaussians is an unsupervised problem?

----------------
Variational methods: Mean field approximation

Consider latent random variable z and visible random variable x. How do we compute the posterior P( z | x_i) or any function of it?
In variational inference, we use an approximating distribution Q( z | x_i; phi) to model the true posterior. This approximating
distribution is usually a simple one like a Gaussian.
(From mackay): The objective to be optimized is the variational free energy, given by :
 E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi) / P( z, x_i; theta ) ) )
equivalently, this is
 E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi) / P( z|x_i; theta ) ) ) - E_over_Q(z|x_i; phi)( log P(x_i; theta) )
= KL( Q(z|x_i; phi) || P( z|x_i; theta ) ) - log P(x_i; theta)
Note that the first term is the KL divergence which can be made zero by making Q(z|x_i; phi) converge to the true posterior.
Thus we have the following variational lower bound on the log likelihood
log P(x_i; theta) = KL( Q(z|x_i; phi) || P( z|x_i; theta ) ) - E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi)/ P(z, x_i; theta) ) )
>= - E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi)/ P(z, x_i; theta) ) ) (negative variational free energy)

Note: the assumption is that it is difficult to compute/evaluate the true posterior P( z | x_i; theta ). it is possible to compute P(x | z_i; theta ), 
P(z; theta)
