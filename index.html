Hello World!!

ILSVRC history:
Classification: Alexnet, Zeiler & Fergus, GoogLeNet (VGG), ResNet 
Localization: Alexnet, OverFeat, VGG, ResNet + RPN
Detection: ?, OverFeat, UvA-Euvision, GoogLeNet, ResNet + Faster-R-CNN

Datasets:
NORB
Caltech 101 / 256
CIFAR 10 / 100
CIFAR 10 - 50K training images, 10K testing images over 10 classes

MNIST
Imagenet - 15 million labeled images / 22000 categories
ILSVRC - 1.2 million labeled images, 50000 validation, 150000 testing. ~1000 images in each of 1000 categories
ILSVRC 2010 - test set labels are available

ILSVRC 2013 - localization considered correct only if > 50% overlap (intersection-over-union). For localization, a single box is
predicted for the predicted class.
detection challenge may have zero objects (i.e. background class). background class makes detection harder.
localization and classification share data. detection has few more data points: images where the objects are smaller, images where
certain objects are absent (can be used for background class)
classification winner was clarifai ~11% (results in overfeat paper)
localization winner was overfeat 29.9%

ILSVRC 2012 and 2013 have the same train and test data? (mentioned in overfeat)

PASCAL VOC 2007 - ~10K images for classification task; 20 object categories; mAP metric is used
PASCAL VOC 2012 - ~22K images for classification task; 20 object categories; mAP metric is used
Caltech 101 - 9K images in 102 categories (101 + a background class)
Caltech 256 - 31K images in 257 categories
PASCAL VOC 2012 Action classification - predict action given a bounding box around a person performing an action. 4.6K images and 11 classes. mAP metric is used.

The way caltech dataset is typically evaluated: Generate random train/test splits and report average performance across them. Mean class recall is used since it compensates for different number of test images per class.


Preprocessing:


Regularization:
Data augmentation - adding noise, cropping, flipping, altering intensity of RGB images
Alexnet style RGB intensity alteration - collect all RGB values (3D vector) from all training images. PCA to compute eigenvectors 
and eigenvalues. Alter each training image by adding all three PCA directions scaled by their eigenvalue * N(0,0.1). Note: all 
pixels in image use the same gaussian sample but different images use different ones.
Dropout - reduces co-adaptation of features. increases the number of iterations needed to converge. e.g. w/ 0.5, alexnet needs
roughly 2X iterations (note: alexnet runs dropout only on two FC layers but these layers have most of the parameters)

Misc:
LRN - local response normalization - post Relu; normalize the activation by L2 norm of activations from 
neighboring kernels (n/2 on either side) raised to beta (hyperparameter). 
This ties the activations together, e.g. two activations can't both be high
Image similarity can be determined by L2 norm on the last hidden layer's representation of image. E.g. when used on ILSVRC, this 
retrieves similar images which in pixel-level-L2 would be far off.
Convolutions over entire image is more efficient than computing entire pipeline for each sliding window due to convolutions 
reusing overlapping computations (see fig 5 of Overfeat)

Optimization:
GD:
SGD:
Momentum:
Nesterov Momentum:
AdaGrad:
RMSProp:
Adam:


conv layer: for 1 based indexing: op dimension = floor( 1 + (ip dimension - kernel dimension + padding) / stride )

Title: ImageNet Classification with Deep Convolutional Neural Networks

Notes: 
60 million params, 650K neurons, dropout regularization, Relu
ILSVRC 2010 & 2012
1.2 million labeled images
5-6 days on 2 GTX580
rescaled img such that shorter side of image is 256. central 256x256 cropped out
subtracted the mean value, over training set, per pixel
LRN - local response normalization - reduces error rates by ~1.2%
Overlapping Pooling - makes it slightly harder to overfit

Architecture:
Relu activations throughout
ip layer: 224 X 224 X 3
kernels:  11 X 11 ; stride 4
layer 2:  55 X 55 X 96
LRN: neighborhood 5, beta 0.75, alpha, k
max pool: 3 x 3; stride 2
kernels:  5 X 5 ; stride 
layer 3:  27 X 27 X 256
LRN: same as above
max pool: 3 X 3; stride 2
kernels: 3 X 3 ; stride
layer 4: 13 X 13 X 384
kernels: 3 X 3 ; stride 
layer 5: 13 X 13 X 384
kernels: 3 X 3 ; stride
layer 6: 13 X 13 X 256
max pool: 3 X 3; stride 2
dropout: 0.5
layer 7: FC 4096
dropout: 0.5
layer 8: FC 4096
op layer: softmax 1000
Data augmentation - 2048 X increase in training set by cropping random 224x224 regions + their horizontal flips.
10 X during testing by considering the four corners, center + their horizontal flips
Learning: SGD, 0.9 momentum, mini-batch 128, 0.0005 weight decay. learning rate set to 0.01 and reduced 10X manually based on 
validation error. during training it went through 3 reductions. 
weight decay is not just regularizing. it helps reduce training error as well.
Initialization: weights are N(0,0.01). biases in layers 3, 5, 6 are set to 1; remaining are set to 0.
Training consumed 90 cycles of the entire training set (1.2 million images)

Results:
ILSVRC 2010: top 1: 37.5%; top 5: 17.0%; top 5 w/o avg-ing over 10 patches: 18.3%
ILSVRC 2012: top 5: 18.2%; 5 CNN avg top 5: 16.4%
Removing any of the conv layers results in 2% loss on top 1 performance.

OverFeat:
single convnet for classification, localization and detection.
avoid training on background class.
ILSVRC 2012 and 2013. Winner of 2013 localization.

Training:
Trained on ILSVRC 2012 (1.2 million images)
Image is rescaled so the smaller dimension is of 256 pixels. 
Extract 5 random crops (221 X 221) and feed them + their horizontal flips in mini-batches of 128.
weights initialized to N(0,0.01)
SGD with momentum 0.6, weight decay 0.00001
Learning rate: 0.05 and reduced 2X at (30, 50, 60, 70, 80) epochs

Architecture:
Training treats architecture as non-spatial (i.e. 1x1 output) but inference treats it as spatial.
Relu activations throughout
(typo in their paper on the first two layers of the fast model?)
Fast model:
ip layer: 221 X 221 X 3
kernels: 11 X 11; stride 4
max pool: 2 X 2; stride 2
layer 1: 28 X 28 X 96
kernels: 5 X 5; stride 1
max pool: 2 X 2; stride 2
layer 2: 12 X 12 X 256
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 12 X 12 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 4: 12 X 12 X 1024
padding: 1 all around
kernels: 3 X 3; stride 1
max pool: 2 X 2; stride 2
layer 5: 6 X 6 X 1024
FC:
layer 6: 1 X 1 X 3072
FC:
layer 7: 1 X 1 X 4096
softmax: 1000 classes

Accurate:
ip layer: 221 X 221 X 3
kernels: 7 X 7; stride 2
max pool: 3 X 3; stride 3
layer 1: 36 X 36 X 96
kernels: 7 X 7; stride 1
max pool: 2 X 2; stride 2
layer 2: 15 X 15 X 256
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 15 X 15 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 4: 15 X 15 X 512
padding: 1 all around
kernels: 3 X 3; stride 1
layer 5: 15 X 15 X 1024
padding: 1 all around
kernels: 3 X 3; stride 1
max pool: 3 X 3; stride 3
layer 6: 5 X 5 X 1024
FC:
layer 7: 1 X 1 X 4096
FC:
layer 8: 1 X 1 X 4096
op layer: 1000 way softmax

Compared to Alexnet (60 million parameters), overfeat has ~ 2X more (~140 million parameters). 
Applies to both the fast, accurate models.

Inference:
applied in a sliding window manner to 6 resolutions + their horizontal flip
Subsampling loss from accurate model is 2 (stride) X 3 (max-pool) X 2 (max-pool) X 3 (max-pool)
Can get rid of the last factor as follows:
apply layer 5 max-pool for all 9 (3X3) offsets of x, y. we obtain 9 versions of layer 5. 
apply rest of the net and obtain 9 versions of classification results (application for each version is in sliding manner, so 
the output will be a spatial classification map). interleave the 9 versions (e.g. x = 0 followed by x = 1, x = 2)
For each resolution + flip, each class's metric is taken as spatial max
the resulting 1000 dimensional vector is averaged across the resolutions and flips
the 6 resolutions have dimensions ranging from 245 to 569 and various aspect ratios

Results:
on ILSVRC 2012
Alexnet: top 5: 18.2
OverFeat fast model: single scale top 5: 16.97; six scales top 5: 16.27; 7 models 4 scales top 5: 13.8

Localization:
uses same feature extractor as classification but has a new regressor network on top of layer 5
layer 5:
FC:
layer 6: 1 X 1 X 4096
FC: 
layer 7: 1 X 1 X 1024
FC:
layer 8: 1 X 1 X 4 (coordinates) ( if done per class: i.e. 1 X 1 X 4000 )

Training:
fix the feature extraction layers (up to layer 5). Train the regressor network using L2 loss on true coordinates. However, do 
not train on bounding boxes that have less than 50% overlap on the field of view. Also, regressor is trained on the 6 scales.

Inference:
Similar to classification, multiple 3 X 3 offsets of layer 5 are evaluated.
Bounding box predictions are combined across scales and spatially to form a set of bounding box candidates. From this set, 
merge candidates are chosen as those who have smallest distance from their center to the center of their overlap. If this 
distance exceeds a threshold, merging stops. The candidates are merged by averaging their corner coordinates.
The class scores for the merged box is the sum of scores associated w/ input window corresponding to each bounding box that
participates in the merge.
per-class-regression (i.e. top layer per class) does worse than single-class-regression. likely due to lack of samples per class.

Detection:
not much detail. re-read.
did not retrain on the validation set. validation set distribution is significantly different enough from training set that it 
results in 1 pt better mAP.
came in 3rd but post-competition were able to improve to 1st. 

suggestions from paper:
backpropagate through all layers for localization. use IOU loss for localization (feasible as long as there is some overlap).
alternate parametrization of bounding box can decorrelate the output and result in better training. 


VGG - Very Deep Convolutional Networks for Large Scale Image Recognition

ILSVRC 2014: second in classification; first in localization

input is 224 X 224 pixels. 
per pixel mean value, across training set, is subtracted from each pixel
all convolutions as 'same' convolutions; i.e. padding of 1 all around for 3 X 3 kernels
LRN (alexnet) does not improve performance on ILSVRC; leads to increased computation and memory

Architecture:
Model E:
ip layer: 224 X 224 X 3
padding: 1 all around
kernels: 3 X 3; stride 1
layer 1: 224 X 224 X 64
layer 2: same as previous layer
max pool: 2 X 2; stride 2
padding: 1 all around
kernels: 3 X 3; stride 1
layer 3: 112 X 112 X 128
layer 4: same as previous layer
max pool: 2 X 2; stride 2
layer 5: 56 X 56 X 256
layer 6: same as previous layer
layer 7: same as previous layer
layer 8: same as previous layer
max pool: 2 X 2; stride 2
layer 9: 28 X 28 X 512
layer 10: same as previous layer
layer 11: same as previous layer
layer 12: same as previous layer
max pool: 2 X 2; stride 2
layer 13: 14 X 14 X 512
layer 14: same as previous layer
layer 15: same as previous layer
layer 16: same as previous layer
max pool: 2 X 2; stride 2
FC:
layer 17: 1 X 1 X 4096
Dropout: 0.5
FC:
layer 18: 1 X 1 X 4096
Dropout: 0.5
FC:
layer 19: 1000 way softmax

total number of parameters is similar to OverFeat (~144 million)

with 3 X 3 kernels; three layers will reach an effective receptive field of 7 X 7 but will have fewer params than a single 
7 X 7 kernel. Also, it has three non-linearities vs. one.

Training:
mini-batch size of 256, momentum of 0.9, weight decay of 0.0005, learning rate initialized to 0.01 and dropped by 10X based 
on validation performance. 74 epochs were run.
despite larger number of parameters (v. Alexnet), it took fewer epochs to converge due to regularization imposed by use of 3 X 3
kernels instead of 11 X 11 etc and due to pre-initialization of certain layers.

Random crops were chosen from rescaled training images. At random, they were horizontally flipped and RGB shift (Alexnet) was 
carried out.

Fixed scale training at 256 and 384: Image is scaled such that smallest side is 256 pixels. Random crops are chosen and modifications made prior to training. Once trained; weights 
are copied over for training with smallest side set to 384 pixels. While training for 384, learning rate is decreased to 0.001
Multi scale training: Image is scaled such that smallest side is a random value between 256 and 512. First the network is 
trained for fixed scale of 384 pixels. Then all layers are fine tuned to adapt to the multi scale setting.

Initialization:
weights ~ N(0, 0.01)
first trained smaller model (A). then reused the layers from A to initialize layers in bigger model. The copied-over layers had
the same learning rate as other layers and hence were allowed to adapt. biases were set to 0 (why?). they later felt Glorot 
initialization would have been equally effective.


Test/Inference:
Image is rescaled so the smallest side is of certain dmension. Network is applied over the entire image resulting in a 
class score map of variable size. this is spatially averaged to get per class scores. similarly, the flipped image is evaluated
and class scores are avged between original and flipped versions.

Training done on 4 Nvidia Titan Black GPUs w/ speedup of 3.75. Data parallelism was achieved by each GPU computing over a 
a fraction of the mini-batch samples. The gradients computed by each GPU is averaged to obtain the full batch gradient. This is
done in a synchronous manner so the computations are the same as if training occured on one GPU.
Training a single net takes 2-3 weeks

Results:
error rate decreases with increasing depth, saturates at 19 layers. 
replacing pairs of 3 X 3 convolutions by a single 5 X 5 convolution layer results in 7% higher error rate.
scale jittering (i.e. randomly choosing a scale value between 256 and 512 and resizing to have the smaller side match that) 
leads to better performance than training on fixed scale value.
Model E with scale jittering achieves 8% top 5 error for single scale evaluation.

Multi-scale evaluation - run inference on several rescaled versions of image and average the resulting class probabilities. 
models trained with fixed size are evaluated with size around the fixed size. model trained with scale jittering are evaluated 
over wider variations in size since the training jitter is of wide variation.

perform comparably to the classification task winner (GoogLeNet) with delta of only 0.1%. GoogLeNet being 6.7% top 5 error.

Localization:
Won the localization challenge. Similar to OverFeat, can run either single-class-regression or per-class-regression. L2 loss is 
used in the regression layer.
Training done for fixed scales. Was initialized to the classification network trained for the same scale. Initial learning rate 
was 0.001. Explored fine-tuning all layers or only the fully connected layers.


Testing (Localization):
Dense application of the localization convent to produce class probabilities and bounding box proposals. Boxes were merged as in OverFeat. When multiple convnets are run, merging is done over the union of box proposals from the various convnets. Did not use the multiple pooling offsets technique of OverFeat which gives finer granularity.

Results:
Per class regression does ~2% better (i.e. error rate). Full fine-tuning does ~1% better than fine-tuning of the fully connected layers alone.
Applying the convnet densely over the whole image improves error by ~6%. Testing at multiple scales and combining predictions of multiple convnets is beneficial.
Results are better than OverFeat by ~5% despite not using as many scales and not running multiple pooling offsets. Thus, we can expect the improvement to have been due to use of better representations by means of deeper net.

Use as feature extractor:
Penultimate layer’s output is treated as image feature. Features are aggregated across locations and scales as follows: Similar to the classification task’s inference, the image is scaled so the smaller side is of certain dimension. The convent is then densely applied to get a 4096 channel spatial map (from the penultimate layer). This is averaged spatially to get a 4096 dimension descriptor. This descriptor is averaged with the descriptor for the horizontally flipped image. Similar descriptors are obtained for multiple scales and either stacked or pooled.
For running classification on other datasets (e.g. PASCAL VOC), the descriptor is L2 normalized and a linear SVM is trained (note: the weights of the convent remain unchanged). 
For PASCAL VOC, didn’t see any difference in stacking descriptors vs. averaging them across scales.
Achieves ~6% better performance on PASCAL VOC compared against others.

For caltech datasets, 3 random test / train splits were used. For each split, the number of images per class was controlled (e.g. for caltech 101, training set had 30 images per class while test set had 50 images per class). For caltech case, stacking the descriptors from different scales was better than pooling. Outperforms state of art by ~8%

On PASCAL VOC 2012 Action classification task; during training (of SVM), features are obtained in two ways - a) descriptor taken over entire image (ignoring the bounding box) b) descriptor taken over bounding box is stacked with the descriptor taken over entire image. Beats state of art by ~8%

VGG has been used in other areas as well - object detection, semantic segmentation, caption generation, texture and material recognition


————————————————————————————————
Andrew Ng Class notes
EM algorithm

Typically for ML estimation, we simply calculate the log likelihood (log P(x; theta)) summed over the independent samples (x1, x2, …) and optimize for theta. 
In case the model is affected by a latent variable, we will need to marginalize over the latent in order to estimate P(x; theta) and this may be intractable.

EM algorithm is applicable in this case. The strategy will be to construct a lower bound (E step) and tighten the bound (M step)

For each sample i, consider that the latent variable obeys a distribution Q_i(z)
log likelihood = sum( log P(x_i; theta) ) >= sum( Expectation_over_Q_i(z)[ log P(x_i, z; theta) / Q_i(z) ] )
for the jensen’s inequality to be tight, we need Q_i(z) proportional to P(x_i, z; theta) for all z; i.e. we need Q_i(z) to be P(z / x_i; theta).
Therefore; log likelihood = sum_over_i( sum_over_z( P(z / x_i; theta) log( P(x_i, z; theta) / P(z / x_i; theta) )))

Then to maximize the log likelihood, we optimize over theta. 
In EM algorithm, however, we run two steps

E-Step: Compute Q_i(z) = P(z / x_i; theta_1)
M-Step: Optimize theta_2 = argmax sum_over_i( sum_over_z ( Q_i(z) log( P(x_i, z; theta) / Q_i(z) )))
Iterate till convergence

Note: in the M-step, Q_i is based on theta_1 - i.e. it is held based on previous value of theta
Note 2: in the M-step, it may look like you are minimizing the KL divergence between Q_i(z) and P(x_i, z; theta)… but notice that with regard to z, P(x_i, z; theta) is not a probability distribution!
Note 3: so, instead of performing marginalization over z; we need to be able to compute P(z / x_i; theta)

elaborating on note 3: why does this matter practically speaking? consider ML estimation for mixture of Gaussian modeling. if you go with the marginalization approach, you will have weighted sum of  Gaussians inside the log - this prevents the exponential from getting cancelled out. Further, when you differentiate w.r.t the mean of the gaussian, you will have a ratio of exponentials as the derivative term and this prevents you from getting to a closed form solution. On the other hand, in the EM algorithm, in the M-step, we treat Q as some constant (having computed it in the E-step). thus the M-step is just like ML estimation where the latent variables are known. In the mixture of Gaussians case, it results in a closed form answer.


Why will the EM algorithm converge?
log likelihood with theta_2 >= sum_over_i( sum_over_z ( Q_i(z; theta_1) log( P(x_i, z; theta_2) / Q_i(z; theta_1) ) )) >= sum_over_i( sum_over_z( Q_i(z; theta_1) log( P(x_i, z; theta_1) / Q_i(z; theta_1) ))) = log likelihood with theta_1 (due to the way Q_i is chosen)

so, with each iteration, you are guaranteed to not get worse; and you are upper bounded by 0

further investigation: look into monte carlo EM (this is when monte carlo methods are applied in the E-step for computing Q(.) due to intractability)

——————————————

Fitting mixture of gaussians is an unsupervised problem?
---------------

auto-encoding variational bayes

See fig. 1 for the directed model discussed in the paper. 

There is a continuous latent variable z and an observed variable x. We want to do ML estimation of parameters. 
Assumption is that marginalizing P(x,z; theta) over z is intractable. Also, computing P(z / x; theta) is intractable (so EM algorithm doesn’t work). Integrals for any reasonable mean-field variational bayes algorithm are also intractable ???

provides solution to: a) estimating theta b) computing posterior P( z / x; theta ) c) efficient approximate marginal inference of x ??? ( i guess they mean generating x? )

Introduce an approximation to the true posterior P( z | x; theta) as Q( z | x; phi). terminology: Q( z | x; phi) is referred to as encoder (z is thought of as a code). P(x | z; theta) is referred to as a decoder.

----------------
Variational methods: Mean field approximation

Consider latent random variable z and visible random variable x. How do we compute the posterior P( z | x_i) or any function of it?
In variational inference, we use an approximating distribution Q( z | x_i; phi) to model the true posterior. This approximating
distribution is usually a simple one like a Gaussian.
(From mackay): The objective to be optimized is the variational free energy, given by :
 E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi) / P( z, x_i; theta ) ) )
equivalently, this is
 E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi) / P( z|x_i; theta ) ) ) - E_over_Q(z|x_i; phi)( log P(x_i; theta) )
= KL( Q(z|x_i; phi) || P( z|x_i; theta ) ) - log P(x_i; theta)
Note that the first term is the KL divergence which can be made zero by making Q(z|x_i; phi) converge to the true posterior.
Thus we have the following variational lower bound on the log likelihood
log P(x_i; theta) = KL( Q(z|x_i; phi) || P( z|x_i; theta ) ) - E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi)/ P(z, x_i; theta) ) )
>= - E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi)/ P(z, x_i; theta) ) ) (negative variational free energy / variational lower bound)

the variational bound can also be written in terms of P(z; theta) and P(x_i | z; theta) as 
KL( Q(z | x_i; phi) || P(z; theta) ) + E_over_Q(z | x_i; phi)( log(P(x_i | z; theta) ) - Here we use P(z) instead of P(x_i|z) in the 
KL divergence since P(x_i|z) is not a probability distribution over z.
The second term is called the expected reconstruction error. Why is this re-write of the bound useful? In some cases, we may be 
able to compute the first term analytically. Then sampling is necessary only for the expected reconstruction. The first term
can be interpreted as a regularizing effect, trying to keep the approximating distribution close to the prior on z.

So, the idea is to minimize the free energy
Note: the assumption is that it is difficult to compute/evaluate the true posterior P( z | x_i; theta ). 
However, it is possible to compute P(x_i | z; theta ), P(z; theta)

Now, we want to do gradient ascent to increase the value of the bound by modifying the variational parameters phi and the generative
parameters theta.

For computation of gradient of terms of the form E_over_Q(z | x_i; phi)( f(z) ); the typical MonteCarlo approach is to compute E_over_Q(z | x_i; phi)( f(z) Gradient_wrt_phi( log( Q(z | x_i; phi) ) ) ). This term has high variance ???why???
A reparametrization is done to work around this issue. Q(z | x_i; phi) is taken to be G_phi( e, x_i ) where G(.) is a deterministic function. e is the source of randomness following a distribution P( e ). E.g. P(e) can be N(0,1). G_phi(e, x_i) can be a linear transformation of e with scaling and offset determined by phi and x_i. Then Q(.) is a Gaussian. This reparametrization leads to easier differentiation w.r.t phi.

Recall the variational bound can be written in two forms:
1 : -E_over_Q(z|x_i; phi)( log( Q(z|x_i; phi)/ P(z, x_i; theta) ) ) 

2: -KL( Q(z | x_i; phi) || P(z; theta) ) + E_over_Q(z | x_i; phi)( log(P(x_i | z; theta) ) 

With the reparametrization trick, these become

3: (1/L) Sum_1_to_L( log( P( z_i_l, x_i; theta ) ) - log( Q( z_i_l | x_i; phi ) ) ) where z_i_l is G_phi( e_l, x_i ) where e_l is sampled from P(e)
4: -KL( Q(z | x_i; phi) || P(z; theta) ) + (1/L) Sum_1_to_L( log( P(x_i | z_i_l; theta) ) ) where z_i_l is G_phi( e_l, x_i ) where e_l is sampled from P(e)

In equation 4, we assume the KL term simplifies into a closed form equation.
The second term of eq 4 has an auto-encoder interpretation. z is generated by sampling e from P(e) and applying G_phi( e, x_i ) for a given x_i. This z is then fed into a decoder to compute P( x_i | z; theta). This is called the reconstruction error in auto-encoder parlance.

Note: both EM and variational approaches have the same initial derivation…. in both cases, the log likelihood is lower bounded by negative of the variational free energy. In the EM case, we can compute the posterior of the latent variable, so this simplifies things.

Since the log likelihood of data x_i = KL( approx posterior || true posterior ) + variational lower bound; improving the variational lower bound will eventually improve the log likelihood of data because for the log likelihood to decrease despite the bound increasing, the KL term should decrease. However, the KL term is bounded below by zero, so it cannot decrease forever.

Variational auto encoder model: The prior P( z; theta ) is taken to be N(0, 1). P(x | z; theta) is taken to be a Gaussian whose parameters are computed by MLP. Note that the true posterior P(z | x; theta) is intractable (so EM algorithm cannot apply).  Q( z | x; phi) is also taken to be a Gaussian whose parameters are determined by another MLP. P( e ) is taken to be a multidimensional N(0, 1) Gaussian. 
e_l is sampled from P( e ); z_i_l is computed as G_phi(e_l, x_i) where the function G_phi is simply a scaling and offset applied to e_l. The parameters of the scaling and offset are determined by an MLP as a function of x_i.
Since the approximate posterior and the prior are both Gaussian, we can compute the KL term (and its derivative) in eq 2 exactly.
phi and theta are the corresponding MLP’s parameters.


Note that in eq 4 or 2; the second term is like the supervised learning objective - i.e. the corresponding MLP is trying to learn to map certain zs to certain type of xs
one MLP is learning to suggest certain Gaussian for each x_i (so, its sort of learning to discriminate? the generative MLP is learning to take samples from that Gaussian and map to the x_i

Results:
For training, a small weight decay term on theta was used. All weights were initialized from N(0, 0.01). Minibatch size was 100. Adagrad was used with step sizes of {0.01, 0.02, 0.1}. L = 1; i.e. only one sample of z is used for the reconstruction error estimate.

For MNIST, 500 hidden units were used; for Frey Faces, 200 was used. Dimension of latent variable was varied between 3 to 200 for MNIST; 2 to 20 for Frey Faces. Higher dimension did not overfit - likely due to the regularizing effect in eq 2/4.

Details of the MLP: consists of one hidden layer. from this hidden layer, you have one head estimating mu (mean of the Gaussian) and another estimating log(variance). For these, there is no non-linearity (since squashing would render the space of possibilities tiny). The activation function used for the hidden layer is tanh(.)

——————————
Tutorial on Variational Auto encoders

Additional notes on top of Auto encoding variational bases: One reason to have the generative process from z to x as probabilistic is that we can obtain gradients and learn. If it was deterministic
 learning would be difficult since in the deterministic case, the result either matches the ground truth or not and doesn’t provide indication of how to adjust the weights to make it match better.

Why is it good enough to use N(0,1) samples as source of randomness? the first few layers of the MLP can model a non-linear function to modify the distribution from N(0,1) into the appropriate distribution of the true latent factors (e.g for MNIST - slant, thickness, size etc.)

From scratch: Consider that you want to sample x from some distribution P( x ). we want P(.) to follow the empirical distribution on x. say we model P( x ) as being a function of a latent variable z (vector) - e.g. in the MNIST case, the latent variables would be the digit, slant, size etc. P( x | z ) is modeled by a MLP. In this case, we can assume z is N(0,1) and let the MLP use its first
few layers to go from N(0,1) to the true distribution over latent factors. the remaining layers will map the latent factors to the output in the following way: P( x | z ) = N(f(z; theta), sigma**2)
so, ideally we’ve got regions of the unit Gaussian z mapped to specific digits. How do we train this? we can run ML updates on theta by sampling z multiple times and taking average of P( x | z ) on these (i.e. monte carlo estimate of the E_over_z( P( x| z) ) = P(x) ). An issue with this is that for same/similar zs, we might at one run train it to map to digit a and another epoch train it to map to digit b and so on. We need some way of segregating z based on the digit we want it to map to - i.e. if we train a region of z to map to digit a, we should train another region of z to map to digit b.

Note that by having the regularization term in eq 2 and 4; we guarantee that the distributions of z that map to the various digits are close to a unit Gaussian. This is important during generation 
time because otherwise, we’d need to know the distribution of z for digit a in order to sample from it and thereby generate digit a. Instead, by keeping it close to unit Gaussian, we can just sample from the unit Gaussian.

In this paper, he claims that the main reason for reparametrization is that it allows the gradients computed on the reconstruction error term to flow back into the MLP responsible for the approximate posterior. This is true since the output of the MLP is mu and sigma terms (of the approximate posterior Q(.)) which connect to the generative MLP through addition and multiplication operations with the unit Gaussian noise e.
However, it is not pointed out that we can also have taken the gradient of the reconstruction error term w.r.t phi by E_over_Q(z | x_i; phi)( log(P(x_i|z; theta)) Gradient_over_phi( log(Q(z|x_i; phi)) ) ). To evaluate the Gradient_over_phi( log(Q(z|x_i; phi))), back propagation can be applied on the encoder (i.e. the one producing z) MLP. As per the Auto encoding variational bayes paper, this is not preferred because this manner of computing the gradient has large variance.

Conditional VAEs

Consider problems like hole filling where a part of an image has been cut out and it needs to be generated, or a string of digits is provided and we need to generate the next one. In these problems, the probability distribution of the feasible answers is multi-modal (e.g. only digits 0, 1, 2,. . . from the space of all images). If you run a regression on this where you are penalizing L2 norm between the prediction and ground truth, you’ll end up training the network to generate the centroid of the high probability points in the feasible space (e.g. an avg image over all digits). What is needed instead is a way of sampling from the multi modal distribution given an incomplete image or string of digits. Since the VAE enables us to sample from a multimodal distribution, we can extend it by conditioning on the incomplete image. Both the MLPs (encoder and decoder) take the incomplete image as an additional input. The approximate posterior is conditioned on the incomplete image and so is the generative probability. The prior on z is taken to be unit Gaussian, i.e. independent of the incomplete image. This is ok because the first few 
layers of the generative MLP will transform the distribution from unit Gaussian to the appropriate one for the latent variables and these layers can alter the result based on the incomplete image which is input.

Results:
MNIST VAE - run in Caffe using Relu and ADAM. For the generated image, each pixel is binarized and probability of being on is modeled with a sigmoid. performance is poor if z (latent) has very few dimensions (e.g. 4) or very large dimensions (e.g. 10000)
MNIST CVAE - conditioning is done on the middle column of pixels. Tried to show example where half the digit is provided and the CVAE completes the remaining half. This works, however a simple regression model also does this well. Likely this is due to lack of multi modal uncertainty given half the digit and/or due to overfitting.
------------------
Visualizing and Understanding Convolutional Networks

They show that transfer learning achieves new records on Caltech-101 and Caltech-256. The softmax layer alone is retrained.

Deconvnet is applied to generate the image corresponding to a particular activation. A layer is chosen and all but one neuron’s output is zero’ed out. The activations are then passed through the deconvnet.
Unpooling: While pooling, we note down the neurons which had the max value and had their activations transferred forward. For unpooling, this mapping is reversed. The ambiguity with this approach is that the non-max neurons’ values are unknown - just that they are less than the max’s.

Rectification: This is inverted by passing through relu again. Note that the activation values are non-negative. Thus, passing through relu to invert is reasonable.

Filtering: This is inverted by applying transposed versions of the filters - i.e the filters are flipped vertically and horizontally. Think of the neurons in the filtered layer being connected to the layer beneath. Filtering with the flipped filter is equivalent to sending the signal back through the same weighted connections.


Architecture:
relu activations throughout
ip layer: 224 X 224 X 3
kernels: 7 X 7; stride 2
intermediate layer: 110 X 110 X 96
max pool: 3 X 3; stride 2
contrast normalization across feature maps
layer 1: 55 X 55 X 96
kernels: 5 X 5; stride 2
intermediate layer: 26 X 26 X 256
max pool: 3 X 3; stride 2
contrast normalization across feature maps
layer 2: 13 X 13 X 256
kernels: 3 X 3; stride 1
layer 3: 13 X 13 X 384
kernels: 3 X 3; stride 1
layer 4: 13 X 13 X 384
kernels: 3 X 3; stride 1
intermediate layer: 13 X 13 X 256
max pool: 3 X 3; stride 2
layer 5: 6 X 6 X 256
FC:
layer 6: 4096
FC:
layer 7: 4096
FC: 
output layer: softmax over classes


Training:
Model was trained on ILSVRC 2012. Each image was rescaled so the smallest side is 256. center 256 X 256 is cropped. This is done for all images and the per pixel mean is subtracted. 10 224 X 224 crops are taken like in Alexnet (center, corners and flips). Mini batch size is 128, learning rate of 0.01
Momentum of 0.9. Learning rate is reduced manually by monitoring the validation error. Dropout is used in the fully connected layers w/ probability 0.5. Weights are initialized to 0.01 (note: not random??) and biases to 0. Trained for 70 epochs - took 12 days on GTX580.
They notice that in the first layer, some of the kernels dominate. These are normalized to have RMS (take root of the mean of the squared values) value clipped to 0.1 if it exceeds this.

Visualization:
For each layer, they look at the top 9 activations from a random subset of the feature maps. They deconv this back to image space and show the corresponding images. They also show the image patches corresponding to these top 9 activations. 
Note that since the deconvolution needs to remember the max switches, it can vary depending on the
image used as input.
By deconving from different layers over time, we see that the upper layers need 40-50 epochs to develop
Under image transformations such as translation, scaling and rotation, the lower layers are highly sensitive but sensitivity reduces as we go up the convnet. This is studied using the L2 loss between the feature maps of the modified image and the feature maps of the original image. The network is more sensitive to rotation than translation or scaling.
Alexnet visualizations show aliasing in layer 2 due to the large stride (4).

Results:
Removing the FC layers in alexnet results in ~4% higher error. Removing the top FC layer results in almost the same performance. Removing layers 3 and 4 also results in ~4% higher error.
Increasing the number of feature maps in the intermediate layers improves performance slightly (for their net, not done on Alexnet). In addition, if they double the number of neurons in the FC layers, they see overfitting.
To evaluate transfer learning, the top softmax layer is replaced and retrained. A linear SVM could be used instead. 
Some of the images overlap between ILSVRC 2012 and Caltech datasets. These were detected through normalized correlation and removed from ILSVRC during training of the convnet in order to avoid train/test contamination.
For caltech-101, picked 30 images per class for training (at random) and 50 images per class for testing. 5 train/test folds were used. per-class accuracies were reported. See reference for details.
Pre-training the convnet on ILSVRC, then applying on Caltech beats best reported result by 2% but starting with an untrained convnet does terribly (roughly half the accuracy)
For caltech-256, picked {15, 30, 45, 60} for training. Similar to caltech-101, pretrained beats the best while starting with untrained performs terribly. the pretrained model needs only 6 images per class to match leading methods using 60 images per class.
For PASCAL 2012, performance is close but doesn’t beat the best. PASCAL images contain multiple objects and may be of full scenes unlike ILSVRC images.
On caltech datasets, they experiment with retaining only the first x layers (adding a SVM or softmax on top) and see that depth provides advantage.

----------------------------------------
GoogLeNet

Driving motivations were to keep computational costs low while reaching higher depth and width. Uses 12X fewer parameters than Alexnet.
Won the ILSVRC 2014 classification challenge.
Budget target for inference time was 1.5 billion multiply adds.
Draws on ideas from ‘Network in Network’ paper.
Main issues with larger network are two fold: more labeled data necessary to train such (i.e. avoiding overfitting) and increased computation.
Inspired by Arora’s results (see reference) they claim a way to overcome these issues is to build a large network that has sparse connectivity. Arora’s result says that neurons that are highly correlated can be clustered (merged??). However, practically speaking, computation machinery is not as optimized for sparse computations as for dense ones like dense matrix multiplication. So, the inception architecture came out of a study into how the sparsity principles can be applied using dense computations. 

So, how can dense components be used to create local sparse structure for convolutional vision networks. < need to read through the references on network-in-network paper and the one by Arora to understand further >

Inception module: We’re not sure what convolution to run on a layer, so we run three types: 1x1, 3x3 and 5x5. Additionally, we also run 3x3 maxpooling. The next layer is simply a concatenation of these four types of feature maps. Note that the convolution layers are followed by relu.

Inception module w/ dimensionality reduction: In order to reduce the number of operations, we place 1x1 convolution layers (+relu) prior to the 3x3 and 5x5 layers in the Inception module. 1x1 conv layers (+relu) are also placed after the 3x3 maxpooling in order to reduce the number of feature maps.

An inception network is a stack of Inception modules. For memory reasons, the lower layers are kept as traditionally conv or pool layers. This is ok since anyway the lower layers only learn simple features. 

Architecture: See table 1

Input is 224 X 224 X 3 with per pixel mean subtraction.
Dropout 0.4 is applied in the fully connected layer.
In order to train the deep network (22 layers); auxiliary classification heads (avg pooling + conv + FC + softmax) were attached to 4a and 4d inception modules. The loss from these auxiliary heads was added to the main loss with a scaling of 0.3. Later experiments showed that you only needed one of the auxiliary heads and that too didn’t result in much of an improvement (less than 1%).

Training was done on distbelief framework with asynchronous SGD having momentum 0.9, fixed learning rate schedule (decrease by 4% every 8 epochs). Polyak averaging was used to create the final model.
For training, it is suggested to use various scales such that the area ranges from 8 to 100% of the original image area. Aspect ratio is suggested to be kept in the interval [3/4, 4/3]. Photometric distortions help reduce overfitting.

Testing used 7 independent models. 144 crops were taken per image: the image was rescaled so that the shorter side is of length {256, 288, 320, 352}. Then left, center, right (or top, center, bottom depending on which side is shorter) squares were extracted. From each square the four corner and center 224 x 224 crop is extracted. Also the square is resized to 224 x 224. Horizontal flips of the crops and resize are also used. 
The softmax output computed for each crop for each model is averaged over all models and crops.
1.5-2% better performance by using multiple crops. 1-2% better performance by using multiple models.

For the detection task, region proposals are done by combining selective search and multibox. 6 GoogLeNet models are used to identify the class in each region. Bounding box regression is not done. Despite this they rank 1st in detection
Since a good portion of localization dataset bounding boxes are not included in the detection dataset, the localization data may be used to pretrain the network’s bounding box regressor just like classification dataset is used to pretrain the classifier. GoogLeNet did not do this though.
—————————————————
Some improvements on deep convolutional neural network based image classification

———————
Deep Residual learning for Image recognition

If we increase the depth of typical convnets, we run into exploding/vanishing gradients problem and this has been addressed to a large extent by normalized initialization (like Glorot initialization) and by introducing batch normalization. Despite this a degradation problem arises wherein beyond a point, adding more layers makes training performance worse. 
Residual networks make use of layers that try to fit a residual mapping ( H(x) - x ) rather than the desired mapping H(x). 

Won ILSVRC 2015 classification, localization and detection. 

The resnet building block has at least two conv layers. The Relu after the second conv layer is performed only after adding x. If the building block only had one conv layer, it would be like a traditional conv layer with some of the weights fixed at 1. They do not observe any advantages in such case. If necessary, the shortcut connection can have a weight matrix multiplication (after flattening) in order to change the number of feature maps to match the output of the residual path (e.g. x is only 3 channels but the residual path outputs 10 feature maps). The addition of the residual and x is done per feature map.

The baseline convnet is inspired by VGGnet. Unlike VGG, it quickly reduces in resolution through use of a stride 2 convolution. For the remaining layers, it adds many more convolution layers at each resolution thereby bringing the total layers to 34.
The Resnet is obtained by adding skip connections to the 34 layer baseline convnet. A skip connection jumps over two convolution layers. At points where the skip connection goes from one resolution to another, the number of feature maps has doubled. To deal with the resolution decrease, either a) zero padding is used (i.e. the new feature maps are zero) or b) 1X1 convolution is performed on x. To match resolution, stride 2 is used.

Image is rescaled so the shorter side is in the interval [256, 480]. A random 224x224 crop is extracted from the rescaled image or its horizontal flip and per pixel mean (over training images) is subtracted. Color augmentation like in Alexnet (using PCA) is applied. Batch normalization is performed between convolution and activation throughout the network. Mini batch size 256, SGD with momentum 0.9 is applied. Learning rate is 0.1 with 10X reduction when the validation plateaus. weight decay is 0.0001. Dropout is not used since batch norm suffices. Trained for roughly 60x10e4 iterations (128 epochs). 
For testing, the network is densely applied and results averaged after rescaling so the shorter side is {224, 256, 384, 480, 640}.

For plain nets, degradation is observed despite use of batch normalization. i.e. 34 deep network has worse training error than 18 deep network. Resnets do not exhibit this issue. For the 18 deep network, it is observed that the resnet converges faster than plain net despite both achieving similar performance. Thus the shortcut connections help SGD. Performing 1x1 convolutions on the shortcut connections is better than zero padding as expected (since zero padding means those feature maps cannot do residual learning)

Bottlenecked building block: To ease computation, a bottlenecked design can be considered where instead of two 3x3 convolutions, we have three convolutions 1x1, 3x3 and 1x1 (the relu on the last one is applied only after adding in the short circuit connection - just like in the non-bottlenecked building block). the first 1x1 reduces the number of feature maps and the last 1x1 restores it back.
Having 1x1 convolutions on the short circuit path will dampen the benefit of bottlenecked building blocks due to the short circuit connecting high feature map points. Thus their use is to be minimized.

Object Detection: Faster R-CNN is used with ResNet-101 replacing VGG-16. Network is first trained on classification dataset, then fine tuned on detection dataset. Remaining TBD - needs background on R-CNN.

CIFAR 10 experiments: 32 x 32 images with per pixel mean subtracted are used. TBD

Note that there are no max pooling layers!

---------------------------------
Some improvements on Deep Convolutional Neural Network based Image Classification

In Alexnet, the image is rescaled so the smaller side is of length 256. Then the central 256x256 area is cropped out and 224x224 crops taken from it. This paper suggest to take random 224x224 crops from the full rescaled image of size 256XN (or NX256).
Python image library is used to modify brightness, contrast and color in the range of [0.5, 1.5]. After this, Alexnet style color modification (i.e. PCA) is done.
For testing, first three views are generated. Image is scaled such that the smaller side is 256. Three views are chosen (left, center, right or top, center, bottom). For each of these 256x256 images, three scales are generated (256, 228, 284 - bicubic interpolation is used for both up and down-scaling) and 5 crops taken from it (like Alexnet). Horizontal flips are also applied. This gives 90 predictions, which may be prohibitively large for real time use. 

Higher resolution model: if the crops are taken from the image scaled to a very different value, the convnet needs to be 
retrained for that new scale. Consider rescaling the image so the smaller side is 448. 224x224 crops are taken from this 
for training. In practice, instead what is done is that 128x128 patches are taken from the smaller-side-256 scaled images. 
128 is used since it is half of 256 (just like 224 is half of 448). The 128x128 patch is upscaled to 224x224 and used to train the convnet. The convnet is initialized to the lower resolution convnet and fine-tuning is done covering all layers. This completes in 30 epochs compared to the 90 needed for the low resolution case.

—————————————————————————————
Polyak averaging:
average the parameters over past time steps; i.e. = (1/T) * sum_i_0toT parameter( i )
—————
How is the mini batch gradient computation implemented? is it that after forward prop, the gradients at the top are initialized (e.g. gradient of softmax w.r.t the logics) and back prop is done from that point on?


