Chapter 5: Machine Learning Basics

deep learning is a type of machine learning

machine learning components: model, dataset, cost function, optimization algorithm.

types of tasks handled by machine learning: 
classification
classification with missing input (e.g. model the joint probability and marginalize as necessary)
regression
transcription - converting from unstructured representation (e.g. image, audio) to text
machine translation - eng to french etc
structured output - output is a vector (or some output containing multiple values). e.g. pixel segmentation, identifying roads in images, identifying parse tree of a sentence, image captioning
anamoly detection - e.g. credit card fraud
synthesis and sampling - e.g. video game texture generation, text to audio
imputation of missing values - fill in values that are missing in a sample
denoising - compute denoised version of sample
density estimation - compute probability density of the sample space p(x)

F1 score = harmonic mean of precision and recall
high precision = low false alarm
high recall = low missed detection

unsupervised learning - learns p(x)
supervised learning - learns p(y|x)

design matrix - a way of representing the dataset; rows are the examples, columns are the features
in case of supervised learning, we also have a set of labels

Linear regression bias term can be modeled by augmenting the samples with an extra feature that takes value of 1. Then the problem
becomes to find weights w such that Xw = Y where the L2 norm of Xw - Y is minimized. i.e. projection of Y onto the column space of 
X.

Y = Xw + Y' where X.T * Y' = 0 (i.e. Y' is in the null space of X.T; because Y' should be orthogonal to any vector in the subspace).
w = (X.T * X)**-1 * X.T * Y

if train and test sets are collected arbitrarily, not much we can do in terms of making predictions for test set.
if samples in train and test are collected i.i.d from the same data generating distribution, the expected training set error = expected test set error if parameters are fixed ahead of time
since parameters aren't fixed by optimized to reduce the training set error, test set error > training set error

machine learning algorithm performance affected by
making training error small - underfitting
making gap between training and testing error small - overfitting
these are controlled by the capacity of the model whose parameters are being learnt. 
low capacity tends to underfit while high capacity tends to overfit.
roughly, capacity is the ability to fit many different functions.
representational capacity - family of functions the model can choose from (e.g. linear)
learning algorithm tries to find the best function within this family, but in practice finds an acceptable one that reduces training error
significantly. taking such factors into account, we have effective capacity.
statistical learning theory shows that the gap between training error and test error is bounded by a factor that grows as the capacity grows
but shrinks as the number of samples increases.

typically the training error will reduce till it hits a lower bound called the bayes error. This quantity represents the error due to
1) the features may be insufficient to fully describe the labels (i.e. labels are affected by other features that aren't captured) 
2) inherent randomness in the mapping between X and y (e.g. given an X, y may take on one of multiple values but our model usually predicts
a particular y) - e.g. in classification; also maybe there is inherent noise in the system

non-parameteric models can provide arbitrarily high capacity
parametric models learn a function that is described by a parameter vector whose size is fixed prior to seeing data
e.g. of non-parametric is nearest-neighbor regression. another example is wrapping a parametric model inside an algorithm that
increases the number of parameters as the training set size increases.

any fixed parametric model with less than optimal capacity will asymptote to an error value higher than the bayes error.
a model might have optimal capacity and still the gap between train and test errors might be high. in this case, more data is needed to 
reduce the gap. e.g. consider the extreme case with just 1-10 samples.

the training error may be less than the bayes error. e.g. consider a polynomial model fit to a linear+gaussian_noise data set. The 
polynomial model will fit to the sampling errors and achieve close to zero MSE. in this case, the testing error is likely to be higher than
the bayes error since the optimal value for a given x is the one given by the linear model.
as the training size increases to infinity, the training error of any fixed capacity model must be at least the bayes error since it won't be
able to memorize/learn the sampling errors.

regularization can be thought of as expressing a preference over the functions in the representational capacity of the model. e.g. 
weight decay expresses preference for functions that have small coefficients.
regularization is any modification we make that is intended to reduce the test error but not the training error.
it is better to use regularization to express preference over the space of functions in the representational capacity than to explicitly
limit the functions in the space by excluding them altogether. in the first case, the algorithm might still choose a complex function if it sees
sufficient benefit (e.g. reduction in error outweights the weight decay penalty)

hyperparameters are parameters that control the model / algorithm which the algorithm does not modify. This may be due to the parameter being difficult
to optimize and hence the algorithm doesn't know how to modify it or more often, it may be that it is not appropriate to modify this parameter 
because it controls model capacity (e.g. layers of mlp) - hence training on it will always result in the model with highest capacity.
This issue is solved by use of a validation set. the validation set is used to 'learn' the best hyperparameters (or can be thought of as model selection)

If dataset size is of concern, k-fold cross validation may be used to estimate the generalization error (train to test or train to val)
Small dataset size is concerning because the estimate of the error will have high variance.

Frequentist view: theres a fixed unknown parameter associated with the data generation. we want to estimate this parameter. 
point estimate is any function of the data. sinc the data is random, the point estimate is a random variable.
bias is the difference between the expectation of the point estimate and the true value of the parameter. the expectation is over the
data (i.e. data generating distribution). an estimator is aymptotically unbiased if the bias term goes to zero as the number of data samples
used by the estimator goes to infinity.
estimating the mean and standard error (i.e. sqrt of variance) of the generalization error: compute the error for each sample in the 
test set. take sample mean as an estimator of the mean generalization error. take sqrt of the sample variance as estimator of the true 
standard error. use gaussian assumption (justified because the test samples are i.i.d?) to generate confidence intervals.

confidence interval: 95% confidence interval is the interval over which prob is 0.95 that the true parameter falls in the interval. 
e.g. consider estimating the mean of a gaussian. the true mean is a non-random parameter. the estimate is a random variable - i.e. it is 
an instance of the gaussian. p( mean is in interval) = p( estimate mean is in interval [true mean - k, true mean + k] ) = 0.95. 
from this we get k = 1.96 * sqrt( variance estimate )

in the case where the test error is measured as a MSE between an estimate and a true value, the MSE can be broken down as bias**2 + variance
(of the estimate)

consistent estimator is one which tends to the true value (almost surely in probability) as the number of data samples increases to infinity. Note
that in this definition, expectation is not taken (contrast with definition of unbiasedness)

maximum likelihood estimation is equivalent to minimizing the KL divergence between the empirical data distribution and the model. 
This is the same as minimizing the cross-entropy between the empirical distribution and the model.
Ideally we want the model to match the true data generating distribution but we don't have access to it. instead we match the data generating
distribution.
ML estimate is consistent and among consistent estimators has the least MSE w.r.t the true parameters (cramer-rao bound). Thus it has the fastest
rate of convergence.
when applied to the conditional probability P(y|X;theta), ML forms the basis for many supervised learning algorithms.
with regularization terms like weight decay, we dont strictly minimize the ML component. we end up with higher bias and lower variance.

In the Bayesian perspective, the observed data is non-random but the true parameter is a random variable. The prior incorporates belief about the 
simplicity of a parameter's distribution, hence acts as a regularizer. In the frequentist approach, the uncertainty about the estimator is 
given by the variance of the estimator. 
The bayesian approach of integrating over the posterior helps reduce overfitting.
MAP approach provides a framework for adding regularizers of the additive form, such as weight decay. not all regularizers can be interpreted
as arising from a MAP style argument though because some regularizations are not of the additive form, and others depend on the data.

supervised learning involves optimizing a parametrized family (i.e. finding best parameters) in order to obtain the best log likelihood
where the probability P(y|X;theta) is modeled. E.g. with linear regression, the probability is given as a Gaussian. with logistic  regression,
y takes value of 0 or 1, probability is given as the logistic function.





SVM:

max gamma
s.t.
  x*w/norm(w) - b >= gamma if y = +1
  x*w/norm(w) - b <= -gamma if y = -1

do the following change of variables: multiply throughout by norm(w) and set b = b*norm(w), gamma = gamma*norm(w)
max gamma / norm(w)
s.t.
  x*w - b >= gamma if y = +1
  x*w - b <= -gamma if y = -1
note: if (w, b, gamma) is a solution, (kw, kb, kgamma) is also a solution

do the following change of variables: set w = w/gamma, b = b/gamma
min norm(w)
s.t.
  x*w - b >= 1 if y = +1
  x*w - b <= -1 if y = -1
This is convex; quadratic problem which can be solved using standard methods

the optimal w will be of the form linear_combination(x1*y1, x2*y2, x3*y3, ... ) with only a few of the coefficients being non-zero
kernels: if x is transformed to a kernel space phi(x) and SVM is done in that space, the classification decision is made based on the
value of phi(x)*w - b = linear_combination(phi(x)*phi(x1)*y1, phi(x)*phi(x2)*y2, ...). With kernels, the product phi(a)*phi(b) may be
computed more efficiently than first transforming a to phi(a), b to phi(b) then doing a dot product. 
e.g. let phi(a) = [a_xa_x, a_xa_y, a_xa_z, a_ya_x, a_ya_y, ...]. phi(a)*phi(b) can be quickly computed as (a*b)**2

the Gaussian kernel is given by K(a,b) = exp( - L2(a-b) / 2*sigma**2 )
in this case, classification is based on linear_combination(K(x,x_i)) - b; i.e. based on how close x is to the sample data points

the above formulation is highly sensitive to outliers
consider the following formulation instead
min L2(w) + C*sum_over_samples(epsilon)
s.t. 
  x*w - b >= (1 - epsilon) if y = +1
  x*w - b <= -(1 - epsilon) if y = -1
  epsilon >= 0


    
    
Bias Variance Tradeoff (ESL)

Consider a model Y = f(X) + epsilon (where epsilon represents zero mean randomness due to unmodeled factors and/or stochasticness)    
Our learnt model is fhat(X). we derive the relation between MSE and bias, variance and irreducible error
In the following equations, all expectations are taken w.r.t the training set samples (i.e. treating them as random variables)
    
MSE = E( (Y - fhat(X))**2 | X = x0 ) 
    = E( (f(X) - fhat(X) + epsilon)**2 | X=x0) 
    = E( (f(X) - fhat(X))**2 + epsilon**2 + 2*epsilon*(f(X)-fhat(X)) | X = x0)
    = E( (f(X) - fhat(X))**2 | X = x0) + variance(epsilon)
    = E( (fhat(X) - E(fhat(X)| X = x0) + E(fhat(X)) - f(X))**2 | X = x0) + variance(epsilon)
    = E( (fhat(X) - E(fhat(X)))**2 | X = x0 ) + E( (f(X) - E(fhat(X)))**2 | X = x0) + 2* E(fhat(X) - E(fhat(X)))*E(f(X) - E(fhat(X))) + variance(epsilon)
    = variance(fhat) + (f(X) - E(fhat(X)))**2 + variance(epsilon)
    = variance(fhat) + bias(fhat)**2 + irreducible error
    
 
PCA:
compute covariance or correlation matrix (really depends on the use case, but in general covariance is used if scales of the different dimensions are similar and correlation if we want to normalize the scales)
perform eigen value decomposition on the matrix to obtain the eigenvector dimensions that capture the most variations
alternatively, the principal directions are the right singular vectors of the design matrix after mean subtraction and if required (i.e. equivalent to use of correlation matrix), scale normalization (i.e. divide by the std dev)

eigen decomposition: x.T * x = W * Lambda * W.T where Lambda is the diagonal eigenvalue matrix.
Take z = x * W where x is the original design matrix (mean and optionally scale normalized). This is rotating the sample points so as to decorrelate them.
The covariance matrix of z = z.T * z = W.T * x.T * x * W = W.T * W * Sigma * W.T * W = Sigma.
Sigma is the diagonal eigenvalue matrix of x.T * x; hence z is uncorrelated. the eigenvalues of x.T * x correspond to the variance along each principal component

    
Chapter 7: Regularization
    
Parameter norm penalties: L2, L1 norms
why does L1 lead to sparsity?
Regularization for underconstrained problems: e.g. in PCA, we need to compute the inverse of the covariance matrix. if we don't have enough independent samples, this is not possible.
    In this case, instead we invert ( X.T * X + alpha * I ), i.e. we add alpha amount of variance along the coordinate directions
Dataset augmentation
Noise robustness
    noise added to hidden layers -> dropout
    noise added to data
    noise added to weights -> causes algorithm to find parameters which are surrounded by flat region (i.e. small perturbation of the parameter value doesnt change the objective function value)
Label smoothing
    for softmax targets, using 0 or 1 may not be best since the algorithm will continue learning larger weights in order to get the softmax to achieve these values. 
    instead the targets are made to be epsilon/(k-1) and 1-epsilon (k s.t. prob sums to 1); which are achievable by the softmax.
Multi-task learning
    a base network is used to obtain a shared representation. multiple 'heads' are attached to the representation to further process this representation to solve various tasks. 
    error over all tasks are minimized such that learning carried out for each one influences the shared representation (i.e. backprop all the way). thus the shared part has learnt from multiple tasks.
    one may also have a head which doesn't solve a task specifically but is used in an unsupervised learning sense.
Early stopping
    can be thought of as a model selection problem
    limits the space of parameters that are explored, so limits the capacity of the model
Parameter tying / sharing
    parameter tying refers to keeping parameters close together (e.g. by using L2 norm)
    parameter sharing refers to having the same values for the parameters
    the convolution operation in CNN is a good example of parameter sharing. this has enabled CNNs to have bigger architectures w/o corresponding increase in the amt of data.
Sparse representation
    Similar to sparsifying the parameters through use of L1 penalty, we can apply L1 to the representation (i.e. the hidden activations)
Bagging / Model averaging
    Create k datasets by sampling w/ replacement from the original dataset
    train a model on each of the k datasets
    merge the predictions from the k models - e.g. averaging
    averaging reduces the variance w/o affecting the bias because fhat(X) becomes avg(fhat_i(X)) where i represents the ith model. E(avg(fhat_i(X))) = E(fhat(X)) as long as all
    the models are the same and only the data set is varied. Thus the bias doesn't change. The variance will reduce as long as the models are uncorrelated (i.e. fhat_i(X) are uncorrelated)
Dropout
    removes non-output units (i.e. zeros out input and/or hidden units)
    uses minibatch based learning. for each minibatch, randomly samples a mask to zero out units. then fwd, backward prop and parameter update are run as usual.
    typically input units are included w/ prob 0.8 and hidden units are included w/ prob 0.5
    in the case of bagging, the models are independent. in dropout, they share parameters.
    in bagging, each model is trained to convergence. in dropout most models aren't sampled (exponential number of possible models) and those that are sampled are only 
    trained for one step. the parameter sharing causes all models to benefit when one is updated.
    during inference, each unit is multiplied by the probability that the unit is included in a model.
    dropout is computationally efficient - during training, for each minibatch we need to sample a mask O(n) and remember this mask till backprop is done. during testing only need to scale the weights
    being a regularization technique, dropout reduces the effective capacity, hence often used in combination with increase in model size to offset this.
    with dropout, the number of iterations increases, usually doubles when mask probability is 0.5.
    dropout causes each hidden unit to learn feature that is good in different contexts.
    the dropout noise can't be additive because the hidden unit would simply learn to output a higher value to make the noise relatively small in comparison
Batch normalization
    enables training set error to improve -> reduces underfitting -> hence a bias reduction technique
    removes need for dropout -> reduces overfitting -> hence a regularization technique


    

